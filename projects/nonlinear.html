<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
    -->
    <title>Numerical Analysis: A Nonlinear Story</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="lin_alg.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
          <h1>Numerical Analysis: A Nonlinear Story</h1>
          <hr>
          <p>Shortcut to this page: <a href="nonlinear.html">alanlam.netlify.com/nonlinear</a></p>
          <h2>A Numbers Store Stores Numbers</h2>
          <p>How does a computer store numbers? Vaguely, we could describe it as putting each digit in a space somewhere in the computer's memory. So `100` would need `3` spaces*. This seems like an easy and obvious idea until we get to numbers like `pi`, which has an infinite number of digits. Does this mean we need an infinite amount of memory to store `pi`? But computers don't have infinite memory, so what happens then?</p>
          <p>*Computers work in binary, so things are a little bit different than this.</p>
          <p><b>Computers store a number by keeping as much of it as it can, and then throwing away the rest.</b> So if a computer can only store at most `8` digits, then it would store `pi` as `3.1415926`. But complications arise when we're dealing with really big or really small numbers. For example, `0.00000000000000045` would be stored as `0.0000000`, which is just `0`. This is a problem because `0.00000000000000045` isn't equal to `0`, but it would be treated as such. A similar problem occurs for `1000000000000000`, which would be stored as `10000000`, which is much much smaller.</p>
          <p>This suggests needing a smarter way to represent the numbers. Enter, scientific notation. With scientific notation, we can represent `0.00000000000000045` as `0.45 xx 10^(-16)` and `1000000000000000` as `1 xx 10^15`. Then we would only need to store the `45` and the `-16` for `0.45 xx 10^(-16)`. And the `1` and the `15` for `1 xx 10^15`.</p>
          <div class="box">
            <p>In these examples, the `45` and the `1` are called the "mantissa" and the `-16` and the `15` are called the "exponents".</p>
            <p>Single-precision computers use 32 bits (4 bytes) to store a number:</p>
            <ul>
              <li><p>1 bit for the sign (to indicate whether the number is positive or negative)</p></li>
              <li><p>8 bits for the exponent</p></li>
              <li><p>23 bits for the mantissa</p></li>
            </ul>
            <p>Double-precision computers use 64 bits (8 bytes) to store a number:</p>
            <ul>
              <li><p>1 bit for the sign</p></li>
              <li><p>11 bits for the exponent</p></li>
              <li><p>52 bits for the mantissa</p></li>
            </ul>
          </div>
          <p>Scientific notation helps make it easier to store numbers, but some complications <em>still</em> pop up. Consider a computer that can only store at most `3` digits in the mantissa. Let's say we wanted to store the number `0.002146`. In scientific notation, it would be `0.2146 xx 10^(-2)`, which has `4` digits in the mantissa. Our computer can only store `3` digits in the mantissa, so how should it be stored? There are 2 options: chopping and rounding.</p>
          <p>Under chopping, `0.2146 xx 10^(-2)` would become `0.214 xx 10^(-2)`. As the name implies, the rest of the digits (after the `3^(rd)`) are "chopped off". Under rounding, `0.2146 xx 10^(-2)` would become `0.215 xx 10^(-2)`.</p>
          <p>From this, we can see that what the computer stores isn't always going to be exactly equal to the original number. The version of the number that the computer stores can be denoted by `fl(x)`, where `x` is the original number.</p>
          <p>So the <b>difference between the original number and the stored number is going to be the error</b>. There are `2` types of errors:</p>
          <ul>
            <li><p>absolute error ` = abs(x - fl(x))`</p></li>
            <li><p>relative error ` = abs(x - fl(x))/abs(x)` (this is often more useful)</p></li>
          </ul>
          <p>Errors are bad, but unavoidable. So it would be helpful to know how bad the relative error could be for any number we want to store.</p>
          <p>Let's say we want to store the number `x` (which is not `0`) on an `n`-digit chopping computer (a computer that stores at most `n` digits and uses the chopping method). Written in scientific notation,</p>
          <div class="math">
            <p>`x = +- 0.d_1\ d_2\ d_3\ ... xx 10^a`</p>
            <p>where `d_i` is the `i^(th)` digit of the mantissa of `x`</p>
          </div>
          <p>Since this computer uses chopping, it would only store the first `n` digits of the mantissa. Written in scientific notation,</p>
          <div class="math">
            <p>`fl(x) = +- 0.d_1\ d_2\ d_3\ ...\ d_n xx 10^a`</p>
          </div>
          <p>We want to compute the relative error, so we need `abs(x - fl(x))` and `abs(x)`.</p>
          <div class="math">
            <p>`abs(x - fl(x)) = 0.ubrace(00...0)_n\ d_(n+1)\ d_(n+2)\ ... xx 10^a`</p>
            <p>`abs(x) = 0.d_1\ d_2\ d_3\ ... xx 10^a`</p>
            <p>relative error = `abs(x - fl(x))/abs(x) = (0.obrace(00...0)^n\ d_(n+1)\ d_(n+2)\ ... xx 10^a)/(0.d_1\ d_2\ d_3\ ... xx 10^a)`</p>
            <p>` = (0.obrace(00...0)^n\ d_(n+1)\ d_(n+2)\ ...)/(0.d_1\ d_2\ d_3\ ...)`</p>
          </div>
          <p>To find out the worst our error could be, we want to make it as large as possible. We do that by making the numerator as big as possible (by letting `d_(n+1) = d_(n+2) = ... = 9`)* and making the denominator as small as possible (by letting `d_1 = d_2 = ... = 0`).</p>
          <p>*`0.999... = 1`</p>
          <div class="math">
            <p>`<= (0.obrace(00...0)^(n-1)1)/(0.100)`</p>
            <p>` = 0.obrace(0...0)^(n-2)1`</p>
            <p>` = 10^(-(n-1))`</p>
            <p>` = 10^(1-n)`</p>
          </div>
          <p>So the relative error — which we can denote by `epsilon` — can be at most `10^(1-n)`. This is the largest error possible for an `n`-digit chopping computer.</p>
          <div class="box">
            <p>For an `n`-digit rounding computer, `epsilon = 1/2 * 10^(1-n)`</p>
            <div class="collapse" id="collapseRelErrorRoundBound">
              <p>Let `x = 0.d_1\ d_2\ ... xx 10^a` be a real number. Suppose `d_(n+1) < 5`. Then `fl(x) = 0.d_1\ d_2\ ... d_n xx 10^a`</p>
              <div class="math">
                <p>relative error = `abs(x - fl(x))/abs(x)`</p>
                <p>`= (0.obrace(0...0)^n\ d_(n+1)\ d_(n+2)\ ... xx 10^a)/(0.d_1\ d_2\ ... xx 10^a)`</p>
                <p>`= (0.obrace(0...0)^n\ d_(n+1)\ d_(n+2)\ ...)/(0.d_1\ d_2\ ...)`</p>
                <p>`<= (0.obrace(0...0)^n49...)/(0.10...)`</p>
                <p>`= (0.obrace(0...0)^n5)/(0.1)`</p>
                <p>`= 0.obrace(0...0)^(n-1)5`</p>
                <p>`= 0.5 xx 10^(-(n-1))`</p>
                <p>`= 1/2 xx 10^(1-n)`</p>
              </div>
              <p>Now suppose `5 <= d_(n+1) <= 9`. Then `fl(x) = 0.d_1\ d_2\ ...\ d_n xx 10^a + 0.obrace(0...0)^(n-1)1 xx 10^a`</p>
              <div class="math">
                <p>relative error = `abs(x - fl(x))/abs(x)`</p>
                <p>`= abs(0.d_1\ d_2\ ...\ d_n\ d_(n+1)\ ... xx 10^a - 0.d_1\ ...\ d_n xx 10^a - 0.obrace(0...0)^(n-1)1 xx 10^a)/abs(0.d_1\ d_2\ ... xx 10^a)`</p>
                <p>`= abs(0.obrace(0...0)^n\ d_(n+1)\ ... - 0.obrace(0...0)^(n-1)1)/abs(0.d_1\ d_2\ ...)`*</p>
                <p>`<= (0.obrace(0...0)^n5)/0.1`</p>
                <p>`= 0.obrace(0...0)^(n-1)5`</p>
                <p>`= 0.5 xx 10^(-(n-1))`</p>
                <p>`= 1/2 xx 10^(1-n)`</p>
              </div>
              <p>*Since `5 <= d_(n+1) <= 9`, we have:</p>
              <div class="math">
                <p>`abs(0.obrace(0...0)^n5 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n5`</p>
                <p>`abs(0.obrace(0...0)^n6 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n4`</p>
                <p>`abs(0.obrace(0...0)^n7 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n3`</p>
                <p>`abs(0.obrace(0...0)^n8 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n2`</p>
                <p>`abs(0.obrace(0...0)^n9 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n1`</p>
              </div>
              <p>So `d_(n+1) = 5` (and `d_(n+2) = d_(n+3) = ... = 0`) gives us the biggest error possible.</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseRelErrorRoundBound">Proof</a>
          </div>
          <h2>Errors in Arithmetic Operations</h2>
          <p>Let's suppose we wanted to add `0.42312` and `0.63546` on a `3`-digit rounding computer. First, the computer would store them as `0.423` and `0.635`. Then, the computer would add those two to get `1.058`*. Since this is a `3`-digit rounding computer, it would say that the answer is `1.06`. In math notation:</p>
          <div class="math">
            <p>`fl(fl(0.42312) + fl(0.63546)) = fl(0.423 + 0.635) = fl(1.058) = 1.06`</p>
          </div>
          <p>Notice how this differs from the actual answer, which is `1.05858`.</p>
          <p>*This is a `4`-digit number, but the computer can only store `3`-digit numbers. It turns out that computers temporarily store results of arithmetic operations using more digits if needed.</p>
          <div class="box">
            <p>For longer strings of operations, the computer performs the operations pairwise. For example:</p>
            <div class="math">
              <p>`x * y * z = color(red)((x * y)) * z`</p>
              <p>the computer does `x*y` first, then multiplies that with `z`</p>
              <p>`= fl(color(red)(fl(fl(x)*fl(y)))*fl(z))`</p>
            </div>
          </div>
          <p>We've established that there are errors in storing numbers. And performing operations with "wrong" numbers will lead to numbers that are "more wrong". But how much error could there be when doing arithmetic operations?</p>
          <p>Let's suppose we wanted to multiply two numbers, `x` and `y`. In the computer, they're stored as `fl(x)` and `fl(y)`. Well, we can write:</p>
          <div class="math">
            <p>`fl(x) = x(1 + delta_1)`</p>
            <p>`fl(y) = y(1 + delta_2)`</p>
            <p>where `abs(delta_1)` and `abs(delta_2)` are the relative errors*</p>
          </div>
          <p>In words: `fl(text(some number))` is the actual value with some error attached to it.</p>
          <div class="math">
            <p>*`fl(x) = x(1 + delta_1)`</p>
            <p>`=> fl(x) = x + xdelta_1`</p>
            <p>`=> fl(x) - x = xdelta_1`</p>
            <p>`=> (fl(x) - x)/x = delta_1`</p>
            <p>`=> abs(fl(x)-x)/abs(x) = abs(delta_1)`</p>
          </div>
          <p>So multiplying those two values would result in:</p>
          <div class="math">
            <p>`fl(fl(x)*fl(y))`</p>
            <p>`= fl(x(1+delta_1)*y(1+delta_2))`</p>
            <p>`= (x(1+delta_1)*y(1+delta_2))(1+delta_3)`</p>
            <p>`= xy(1 + delta)`</p>
            <p>where `abs(delta)` is the relative error</p>
            <p>and `(1 + delta) = (1 + delta_1)(1 + delta_2)(1 + delta_3)`</p>
            <p>`= 1 + delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
          </div>
          <p>So how big is the relative error?</p>
          <div class="math">
            <p>`1 + delta = 1 + delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
            <p>`=> delta = delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
            <p>`=> abs(delta) = abs(delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3)`</p>
            <p>`=> abs(delta) <= abs(delta_1) + abs(delta_2) + abs(delta_3) + abs(delta_1delta_2) + abs(delta_1delta_3) + abs(delta_2delta_3) + abs(delta_1delta_2delta_3)` (by triangle inequality)</p>
          </div>
          <p>In the previous section, we defined `epsilon` to be the largest possible relative error. So all of these relative errors are smaller than `epsilon`, i.e. `abs(delta_1), abs(delta_2), abs(delta_3) <= epsilon`</p>
          <div class="math">
            <p>`abs(delta) <= abs(delta_1) + abs(delta_2) + abs(delta_3) + abs(delta_1delta_2) + abs(delta_1delta_3) + abs(delta_2delta_3) + abs(delta_1delta_2delta_3)`</p>
            <p>`=> abs(delta) <= epsilon + epsilon + epsilon + epsilon^2 + epsilon^2 + epsilon^2 + epsilon^3`</p>
            <p>`= 3epsilon + 3epsilon^2 + epsilon^3`</p>
            <p>`= 3epsilon + O(epsilon^2) = O(epsilon)`</p>
          </div>
          <p>So multiplying two numbers will lead to error, but it will be at most `3` times as bad as the worst error in storing a number.</p>
          <div class="box">
            <p>`(x*y)*z` gets relative error `abs(delta) <= 5epsilon + O(epsilon^2)`</p>
            <p>`((x_1*x_2)*x_3)*x_4*...*x_m` gets relative error `abs(delta) <= (2m-1)epsilon + O(epsilon^2)`</p>
          </div>
          <p>Now, let's suppose we wanted to subtract two numbers `x` and `y`.</p>
          <div class="math">
            <p>`fl(fl(x)-fl(y)) = (x(1+delta_1) - y(1+delta_2))(1+delta_3)`</p>
            <p>`= (x - y + xdelta_1 - ydelta_2)(1 + delta_3)`</p>
            <p>`= (x - y)(1 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y))(1 + delta_3)`</p>
            <p>`= (x - y)(1 + delta)`</p>
            <p>where `(1 + delta) = (1 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y))(1 + delta_3)`</p>
            <p>`=> delta = delta_3 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y) + delta_3((xdelta_1)/(x-y) - (ydelta_2)/(x-y))`</p>
          </div>
          <p><b>But `abs(x/(x-y))` and `abs(y/(x-y))` can be very large when `x` is close to `y`. When this happens, `abs(delta)` will be very large. This is called catastrophic cancellation.</b></p>
          <div class="box">
            <p>Example: For `n = 5` (a `5`-digit rounding computer), let's subtract `x = 1` and `y = 1.00001`. (The exact solution is `1 - 1.00001 = -0.00001`)</p>
            <p>`fl(1) = 1` and `fl(1.00001) = 1.0000 = 1`</p>
            <p>So `fl(1) - fl(1.00001) = 0`</p>
            <p>Relative error = `abs(text(exact) - text(approximate))/abs(exact) = abs(-0.00001 - 0)/abs(-0.00001) = 1`</p>
            <p>This means there is `100%` error.</p>
          </div>
          <p><b>1) Errors become slightly larger every arithmetic operation</b>.</p>
          <div class="box">
            <p>Because of this, we want to minimize the number of arithmetic operations whenever possible. For example, `1 + x + x^2 + x^3` takes `6` operations to do (`3` additions, `3` multiplications). However, we could rewrite `1 + x + x^2 + x^3` as `1 + x(1 + x(1 + x))`, which only requires `5` operations (`3` additions, `2` multiplications). The number of operations saved writing polynomials in nested form increases as the number of polynomials increases.</p>
          </div>
          <p><b>2) Catastrophic cancellation can occur when subtracting numbers</b>.</p>
          <div class="box">
            <p>Consider the situation of trying to find the roots of a quadratic equation `ax^2 + bx + c = 0`. Let's suppose `b` is positive and `a`, `c` are small so that `abs(-4ac)` is small. Then `x = (-b + sqrt(b^2-4ac))/(2a) ~~ (-b + sqrt(b^2))/(2a) = (-b + b)/(2a) = 0`*, which leads to catastrophic cancellation. We could avoid this by rewriting `x` by multiplying it by `1`.</p>
            <p>`x = (-b + sqrt(b^2-4ac))/(2a) * (-b-sqrt(b^2-4ac))/(-b-sqrt(b^2-4ac)) = (b^2 - (b^2 - 4ac))/(2a(-b-sqrt(b^2-4ac))) = (2c)/(-b-sqrt(b^2-4ac))`. In this case, catastrophic cancellation is avoided.</p>
            <p>*It only equals `0` when the number of digits of `b` exceeds the number of digits the computer can store. After all, `sqrt(b^2 - 4ac) ~~ sqrt(b^2)`.</p>
          </div>
          <div class="box">
            <p>Catastrophic cancellation only occurs when the number of digits exceeds the number of digits of a number a computer can store. This is why everything is fine when doing something like `5 - 4.99999`.</p>
          </div>
          <h2>Solving Nonlinear Equations</h2>
          <p>How do we solve something like `cos(x) = x`? Let's say we didn't have a calculator, so we can't find the exact answer. How can we find a (good) approximation? In general, how do we find an approximation for `f(x) = g(x)`?</p>
          <h3>Getting to the Root of the Problem</h3>
          <p>One way is to change the equation to the form `f(x) = 0`. Then, finding the roots of `f(x)` will be the solutions. Of course, it isn't going to be obvious what the roots are just from looking at something like `cos(x) - x = 0`. But we can try to trap a root in a small interval (i.e. find a small interval between which a root lies).</p>
          <p>This can be done with an iterative algorithm. We start with a root trapped in a large interval and reduce the size of the interval at every step. But how do we find an interval in which there is a root? Without knowing this, we can't start the algorithm!</p>
          <p>We do this by using the <b>Intermediate Value Theorem, which says if `f` is a continuous function in `[a,b]`, then `f` assumes every value between `f(a)` and `f(b).`</b></p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/ivt_general.jpg">
          </div>
          <p>In this example, `f` achieves every value between the dotted lines between `a` and `b`.</p>
          <p>So how does this help us find roots? We can apply it to the situation where `f(a)` and `f(b)` have different signs (i.e. one is positive and the other is negative). <b>If `f(a)` and `f(b)` have different signs, then `f` has a root in `[a, b]`.</b></p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/ivt_root.jpg">
          </div>
          <p>This suggests that we start the algorithm by picking `a_0, b_0` such that `f(a_0), f(b_0)` have different signs.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_0.jpg">
          </div>
          <p>Now we have to reduce the size of the interval. We can split it in half; then, the root has to either be on the left half or the right half. So let's split it at `c_0` (the midpoint) and suppose that `f(c_0)` is positive.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_0.5.jpg">
          </div>
          <p>Now we have two choices for what our new interval `[a_1, b_1]` could be: `[a_0, c_0]` or `[c_0, b_0]`. The IVT doesn't tell us if there's a root between `a_0` and `c_0`, but it does guarantee that there is a root between `c_0` and `b_0`. So the new interval `[a_1, b_1]` should be `[c_0, b_0]`.</p>
          <p>And this process of dividing the interval in half repeats until the interval is small enough.</p>
        </div> <!-- col -->
      </div> <!-- row -->
    </div> <!-- container -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
