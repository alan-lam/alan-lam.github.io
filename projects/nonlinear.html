<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
    -->
    <title>Numerical Analysis: A Nonlinear Story</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="lin_alg.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
          <h1>Numerical Analysis: A Nonlinear Story</h1>
          <hr>
          <p>Shortcut to this page: <a href="nonlinear.html">alanlam.netlify.com/nonlinear</a></p>
          <p>This is the nonlinear version of the <a href="lin_alg.html" target="_blank">linear algebra story</a>. The linear story was looking at how to solve problems of the form `Ax = b` i.e. `Ax - b = 0`, which is linear. This story looks at how to solve problems of the form `f(x) = g(x)` where `f(x), g(x)` are nonlinear. For example, they could be `x^2, x^3, cos(x), e^(x), ...`</p>
          <h2>A Numbers Store Stores Numbers</h2>
          <p>How does a computer store numbers? Vaguely, we could describe it as putting each digit in a space somewhere in the computer's memory. So `100` would need `3` spaces`color(red)(text(*))`. This seems like an easy and obvious idea until we get to numbers like `pi`, which has an infinite number of digits. Does this mean we need an infinite amount of memory to store `pi`? But computers don't have infinite memory, so what happens then?</p>
          <p>`color(red)(text(*)`Computers work in binary, so things are a little bit different than this.</p>
          <p><b>Computers store a number by keeping as much of it as it can, and then throwing away the rest.</b> So if a computer can only store at most `8` digits, then it would store `pi` as `3.1415926`. But complications arise when we're dealing with really big or really small numbers. For example, `0.00000000000000045` would be stored as `0.0000000`, which is just `0`. This is a problem because `0.00000000000000045` isn't equal to `0`, but it would be treated as such. A similar problem occurs for `1000000000000000`, which would be stored as `10000000`, which is much much smaller.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/comic_storing.jpg">
          </div>
          <p>This suggests needing a smarter way to represent the numbers. Enter, scientific notation. With scientific notation, we can represent `0.00000000000000045` as `0.45 xx 10^(-16)` and `1000000000000000` as `1 xx 10^15`. Then we would only need to store the `45` and the `-16` for `0.45 xx 10^(-16)`. And the `1` and the `15` for `1 xx 10^15`.</p>
          <div class="box">
            <p>In these examples, the `45` and the `1` are called the "mantissa" and the `-16` and the `15` are called the "exponents".</p>
            <p>Single-precision computers use 32 bits (4 bytes) to store a number:</p>
            <ul>
              <li><p>1 bit for the sign (to indicate whether the number is positive or negative)</p></li>
              <li><p>8 bits for the exponent</p></li>
              <li><p>23 bits for the mantissa</p></li>
            </ul>
            <p>Double-precision computers use 64 bits (8 bytes) to store a number:</p>
            <ul>
              <li><p>1 bit for the sign</p></li>
              <li><p>11 bits for the exponent</p></li>
              <li><p>52 bits for the mantissa</p></li>
            </ul>
          </div>
          <p>Scientific notation helps make it easier to store numbers, but some complications <em>still</em> pop up. Consider a computer that can only store at most `3` digits in the mantissa. Let's say we wanted to store the number `0.002146`. In scientific notation, it would be `0.2146 xx 10^(-2)`, which has `4` digits in the mantissa. Our computer can only store `3` digits in the mantissa, so how should it be stored? There are 2 options: chopping and rounding.</p>
          <p>Under chopping, `0.2146 xx 10^(-2)` would become `0.214 xx 10^(-2)`. As the name implies, the rest of the digits (after the `3^(rd)`) are "chopped off". Under rounding, `0.2146 xx 10^(-2)` would become `0.215 xx 10^(-2)`.</p>
          <p>From this, we can see that what the computer stores isn't always going to be exactly equal to the original number. The version of the number that the computer stores can be denoted by `fl(x)`, where `x` is the original number.</p>
          <p>So the <b>difference between the original number and the stored number is going to be the error</b>. There are `2` types of errors:</p>
          <ul>
            <li><p>absolute error ` = abs(x - fl(x))`</p></li>
            <li><p>relative error ` = abs(x - fl(x))/abs(x)` (this is often more useful)</p></li>
          </ul>
          <p>Errors are bad, but unavoidable. So it would be helpful to know how bad the relative error could be for any number we want to store.</p>
          <p>Let's say we want to store the number `x` (which is not `0`) on an `n`-digit chopping computer (a computer that stores at most `n` digits and uses the chopping method). Written in scientific notation,</p>
          <div class="math">
            <p>`x = +- 0.d_1\ d_2\ d_3\ ... xx 10^a`</p>
            <p>where `d_i` is the `i^(th)` digit of the mantissa of `x`</p>
          </div>
          <p>Since this computer uses chopping, it would only store the first `n` digits of the mantissa. Written in scientific notation,</p>
          <div class="math">
            <p>`fl(x) = +- 0.d_1\ d_2\ d_3\ ...\ d_n xx 10^a`</p>
          </div>
          <p>We want to compute the relative error, so we need `abs(x - fl(x))` and `abs(x)`.</p>
          <div class="math">
            <p>`abs(x - fl(x)) = 0.ubrace(00...0)_n\ d_(n+1)\ d_(n+2)\ ... xx 10^a`</p>
            <p>`abs(x) = 0.d_1\ d_2\ d_3\ ... xx 10^a`</p>
            <p>relative error = `abs(x - fl(x))/abs(x) = (0.obrace(00...0)^n\ d_(n+1)\ d_(n+2)\ ... xx 10^a)/(0.d_1\ d_2\ d_3\ ... xx 10^a)`</p>
            <p>` = (0.obrace(00...0)^n\ d_(n+1)\ d_(n+2)\ ...)/(0.d_1\ d_2\ d_3\ ...)`</p>
          </div>
          <p>To find out the worst our error could be, we want to make it as large as possible. We do that by making the numerator as big as possible (by letting `d_(n+1) = d_(n+2) = ... = 9`) and making the denominator as small as possible (by letting `d_1 = 1` `color(red)(text(*))` and `d_2 = d_3 = ... = 0`).</p>
          <p>`color(red)(text(*))``d_1` cannot be `0` because of the way numbers are written in scientific notation. You wouldn't write something like `0.083 xx 10^3`; it would be `0.83 xx 10^2`.</p>
          <div class="math">
            <p>`<= (0.obrace(00...0)^(n-1)1color(red)(text(*)))/(0.100)`</p>
            <p>` = 0.obrace(0...0)^(n-2)1`</p>
            <p>` = 10^(-(n-1))`</p>
            <p>` = 10^(1-n)`</p>
          </div>
          <p>`color(red)(text(*))``0.999... = 1`</p>
          <p>So the relative error — which we can denote by `epsilon` — can be at most `10^(1-n)`. This is the largest error possible for an `n`-digit chopping computer.</p>
          <div class="box">
            <p>For an `n`-digit rounding computer, `epsilon = 1/2 * 10^(1-n)`</p>
            <div class="collapse" id="collapseRelErrorRoundBound">
              <p>Let `x = 0.d_1\ d_2\ ... xx 10^a` be a real number. Suppose `d_(n+1) < 5`. Then `fl(x) = 0.d_1\ d_2\ ... d_n xx 10^a`</p>
              <div class="math">
                <p>relative error = `abs(x - fl(x))/abs(x)`</p>
                <p>`= (0.obrace(0...0)^n\ d_(n+1)\ d_(n+2)\ ... xx 10^a)/(0.d_1\ d_2\ ... xx 10^a)`</p>
                <p>`= (0.obrace(0...0)^n\ d_(n+1)\ d_(n+2)\ ...)/(0.d_1\ d_2\ ...)`</p>
                <p>`<= (0.obrace(0...0)^n49...)/(0.10...)`</p>
                <p>`= (0.obrace(0...0)^n5)/(0.1)`</p>
                <p>`= 0.obrace(0...0)^(n-1)5`</p>
                <p>`= 0.5 xx 10^(-(n-1))`</p>
                <p>`= 1/2 xx 10^(1-n)`</p>
              </div>
              <p>Now suppose `5 <= d_(n+1) <= 9`. Then `fl(x) = 0.d_1\ d_2\ ...\ d_n xx 10^a + 0.obrace(0...0)^(n-1)1 xx 10^a`</p>
              <div class="math">
                <p>relative error = `abs(x - fl(x))/abs(x)`</p>
                <p>`= abs(0.d_1\ d_2\ ...\ d_n\ d_(n+1)\ ... xx 10^a - 0.d_1\ ...\ d_n xx 10^a - 0.obrace(0...0)^(n-1)1 xx 10^a)/abs(0.d_1\ d_2\ ... xx 10^a)`</p>
                <p>`= abs(0.obrace(0...0)^n\ d_(n+1)\ ... - 0.obrace(0...0)^(n-1)1)/abs(0.d_1\ d_2\ ...)color(red)(text(*)`</p>
                <p>`<= (0.obrace(0...0)^n5)/0.1`</p>
                <p>`= 0.obrace(0...0)^(n-1)5`</p>
                <p>`= 0.5 xx 10^(-(n-1))`</p>
                <p>`= 1/2 xx 10^(1-n)`</p>
              </div>
              <p>`color(red)(text(*))` Since `5 <= d_(n+1) <= 9`, we have:</p>
              <div class="math">
                <p>`abs(0.obrace(0...0)^n5 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n5`</p>
                <p>`abs(0.obrace(0...0)^n6 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n4`</p>
                <p>`abs(0.obrace(0...0)^n7 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n3`</p>
                <p>`abs(0.obrace(0...0)^n8 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n2`</p>
                <p>`abs(0.obrace(0...0)^n9 - 0.obrace(0...0)^(n-1)1) = 0.obrace(0...0)^n1`</p>
              </div>
              <p>So `d_(n+1) = 5` (and `d_(n+2) = d_(n+3) = ... = 0`) gives us the biggest error possible.</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseRelErrorRoundBound">Proof</a>
          </div>
          <div class="box">
            <p><b>The story thus far:</b> When we're working with really small/large numbers, computers won't be able to store them exactly as they are. Computers get around this by storing them using scientific notation. But even then, some numbers have so many digits (e.g. `pi`) that computers still can't store them exactly as they are. So computers either round the number or chop the number so there are less digits to store. This means that computers store a modified version of the number (denoted `fl(x)`).</p>
          </div>
          <h2>Errors in Arithmetic Operations</h2>
          <p>Let's suppose we wanted to add `0.42312` and `0.63546` on a `3`-digit rounding computer. First, the computer would store them as `0.423` and `0.635`. Then, the computer would add those two to get `1.058``color(red)(text(*))`. Since this is a `3`-digit rounding computer, it would say that the answer is `1.06`. In math notation:</p>
          <div class="math">
            <p>`fl(fl(0.42312) + fl(0.63546)) = fl(0.423 + 0.635) = fl(1.058) = 1.06`</p>
          </div>
          <p>Notice how this differs from the actual answer, which is `1.05858`.</p>
          <p>`color(red)(text(*))` This is a `4`-digit number, but the computer can only store `3`-digit numbers. It turns out that computers temporarily store results of arithmetic operations using more digits if needed.</p>
          <div class="box">
            <p>For longer strings of operations, the computer performs the operations pairwise. For example:</p>
            <div class="math">
              <p>`x * y * z = color(red)((x * y)) * z`</p>
              <p>the computer does `x*y` first, then multiplies that with `z`</p>
              <p>`= fl(color(red)(fl(fl(x)*fl(y)))*fl(z))`</p>
            </div>
          </div>
          <p>We've established that there are errors in storing numbers. And performing operations with "wrong" numbers will lead to numbers that are "more wrong". But how much error could there be when doing arithmetic operations?</p>
          <p>Let's suppose we wanted to multiply two numbers, `x` and `y`. In the computer, they're stored as `fl(x)` and `fl(y)`. Well, we can write:</p>
          <div class="math">
            <p>`fl(x) = x(1 + delta_1)`</p>
            <p>`fl(y) = y(1 + delta_2)`</p>
            <p>where `abs(delta_1)` and `abs(delta_2)` are the relative errors`color(red)(text(*))`</p>
          </div>
          <p>In words: `fl(text(some number))` is the actual value with some error attached to it.</p>
          <div class="box">
            <p>`color(red)(text(*))` Recall that the formula for relative error is `abs(fl(x)-x)/abs(x)`</p>
            <div class="math">
              <p>`fl(x) = x(1 + delta_1)`</p>
              <p>`=> fl(x) = x + xdelta_1`</p>
              <p>`=> fl(x) - x = xdelta_1`</p>
              <p>`=> (fl(x) - x)/x = delta_1`</p>
              <p>`=> abs(fl(x)-x)/abs(x) = abs(delta_1)`</p>
            </div>
            <p>so `abs(delta_1)` is the relative error (similarly for `abs(delta_2)`)</p>
          </div>
          <p>So multiplying those two values would result in:</p>
          <div class="math">
            <p>`fl(fl(x)*fl(y))`</p>
            <p>`= fl(x(1+delta_1)*y(1+delta_2))`</p>
            <p>`= (x(1+delta_1)*y(1+delta_2))(1+delta_3)`</p>
            <p>`= xy(1 + delta)`</p>
            <p>where `abs(delta)` is the relative error</p>
            <p>and `(1 + delta) = (1 + delta_1)(1 + delta_2)(1 + delta_3)`</p>
            <p>`= 1 + delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
          </div>
          <p>So how big is the relative error?</p>
          <div class="math">
            <p>`1 + delta = 1 + delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
            <p>`=> delta = delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3`</p>
            <p>`=> abs(delta) = abs(delta_1 + delta_2 + delta_3 + delta_1delta_2 + delta_1delta_3 + delta_2delta_3 + delta_1delta_2delta_3)`</p>
            <p>`=> abs(delta) <= abs(delta_1) + abs(delta_2) + abs(delta_3) + abs(delta_1delta_2) + abs(delta_1delta_3) + abs(delta_2delta_3) + abs(delta_1delta_2delta_3)` (by triangle inequality)</p>
          </div>
          <p>In the previous section, we defined `epsilon` to be the largest possible relative error. So all of these relative errors are smaller than `epsilon`, i.e. `abs(delta_1), abs(delta_2), abs(delta_3) <= epsilon`</p>
          <div class="math">
            <p>`abs(delta) <= abs(delta_1) + abs(delta_2) + abs(delta_3) + abs(delta_1delta_2) + abs(delta_1delta_3) + abs(delta_2delta_3) + abs(delta_1delta_2delta_3)`</p>
            <p>`=> abs(delta) <= epsilon + epsilon + epsilon + epsilon^2 + epsilon^2 + epsilon^2 + epsilon^3`</p>
            <p>`= 3epsilon + 3epsilon^2 + epsilon^3`</p>
            <p>`= 3epsilon + O(epsilon^2) = O(epsilon)`</p>
          </div>
          <p>So <b>multiplying two numbers will lead to error, but it will be at most `3` times as bad as the worst error in storing a number.</b></p>
          <div class="box">
            <p>`(x*y)*z` gets relative error `abs(delta) <= 5epsilon + O(epsilon^2)`</p>
            <p>`((x_1*x_2)*x_3)*x_4*...*x_m` gets relative error `abs(delta) <= (2m-1)epsilon + O(epsilon^2)`</p>
          </div>
          <p>Now, let's suppose we wanted to subtract two numbers `x` and `y`.</p>
          <div class="math">
            <p>`fl(fl(x)-fl(y)) = (x(1+delta_1) - y(1+delta_2))(1+delta_3)`</p>
            <p>`= (x - y + xdelta_1 - ydelta_2)(1 + delta_3)`</p>
            <p>`= (x - y)(1 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y))(1 + delta_3)`</p>
            <p>`= (x - y)(1 + delta)`</p>
            <p>where `(1 + delta) = (1 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y))(1 + delta_3)`</p>
            <p>`=> delta = delta_3 + (xdelta_1)/(x-y) - (ydelta_2)/(x-y) + delta_3((xdelta_1)/(x-y) - (ydelta_2)/(x-y))`</p>
          </div>
          <p><b>But `abs(x/(x-y))` and `abs(y/(x-y))` can be very large when `x` is close to `y`. When this happens, `abs(delta)` will be very large. This is called catastrophic cancellation.</b></p>
          <div class="box">
            <p>Example: For `n = 5` (a `5`-digit rounding computer), let's subtract `x = 1` and `y = 1.00001`. (The exact solution is `1 - 1.00001 = -0.00001`)</p>
            <p>`fl(1) = 1` and `fl(1.00001) = 1.0000 = 1`</p>
            <p>So `fl(1) - fl(1.00001) = 0`</p>
            <p>Relative error = `abs(text(exact) - text(approximate))/abs(exact) = abs(-0.00001 - 0)/abs(-0.00001) = 1`</p>
            <p>This means there is `100%` error.</p>
          </div>
          <p><b>1) Errors become slightly larger every arithmetic operation</b>.</p>
          <div class="box">
            <p>Because of this, we want to minimize the number of arithmetic operations whenever possible. For example, `1 + x + x^2 + x^3` takes `6` operations to do (`3` additions, `3` multiplications). However, we could rewrite `1 + x + x^2 + x^3` as `1 + x(1 + x(1 + x))`, which only requires `5` operations (`3` additions, `2` multiplications). The number of operations saved writing polynomials in nested form increases as the number of polynomials increases.</p>
          </div>
          <p><b>2) Catastrophic cancellation can occur when subtracting numbers</b>.</p>
          <div class="box">
            <p>Consider the situation of trying to find the roots of a quadratic equation `ax^2 + bx + c = 0`. Let's suppose `b` is positive and `a`, `c` are small so that `abs(-4ac)` is small. Then `x = (-b + sqrt(b^2-4ac))/(2a) ~~ (-b + sqrt(b^2))/(2a) = (-b + b)/(2a) = 0``color(red)(text(*))`, which leads to catastrophic cancellation. We could avoid this by rewriting `x` by multiplying it by `1`.</p>
            <p>`x = (-b + sqrt(b^2-4ac))/(2a) * (-b-sqrt(b^2-4ac))/(-b-sqrt(b^2-4ac)) = (b^2 - (b^2 - 4ac))/(2a(-b-sqrt(b^2-4ac))) = (2c)/(-b-sqrt(b^2-4ac))`. In this case, catastrophic cancellation is avoided.</p>
            <p>`color(red)(text(*))`It only equals `0` when the number of digits of `b` exceeds the number of digits the computer can store. After all, `sqrt(b^2 - 4ac) ~~ sqrt(b^2)`.</p>
          </div>
          <div class="box">
            <p>Catastrophic cancellation only occurs when the number of digits exceeds the number of digits of a number a computer can store. This is why everything is fine when doing something like `5 - 4.99999`.</p>
          </div>
          <div class="box">
            <p><b>The story thus far:</b> Computers store modified versions of numbers. This means that the number the computer stores is "wrong". When we work with "wrong" numbers (e.g. add, multiply them), we're gonna get an answer that is even "more wrong". For multiplication, things aren't too bad. But for subtraction, we can get undesired results when the numbers we're subtracting are really close to each other.</p>
          </div>
          <h2>Solving Nonlinear Equations</h2>
          <p>How do we solve something like `cos(x) = x`? Let's say we didn't have a calculator, so we can't find the exact answer. How can we find a (good) approximation? In general, how do we find an approximation for `f(x) = g(x)`?</p>
          <h3>Getting to the Root of the Problem</h3>
          <p>One way is to change the equation to the form `f(x) = 0`. Then, finding the roots of `f(x)` will be the solutions. Of course, it isn't going to be obvious what the roots are just from looking at something like `cos(x) - x = 0`. But we can try to trap a root in a small interval (i.e. find a small interval between which a root lies).</p>
          <p>This can be done with an iterative algorithm. We start with a root trapped in a large interval and reduce the size of the interval at every step. But how do we find an interval in which there is a root? Without knowing this, we can't start the algorithm!</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/comic_bisection.jpg">
          </div>
          <p>We do this by using the <b>Intermediate Value Theorem, which says if `f` is a continuous function in `[a,b]`, then `f` assumes every value between `f(a)` and `f(b).`</b></p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/ivt_general.jpg">
          </div>
          <p>In this example, `f` achieves every value between the dotted lines between `a` and `b`.</p>
          <p>So how does this help us find roots? We can apply it to the situation where `f(a)` and `f(b)` have different signs (e.g. one is positive and the other is negative). <b>If `f(a)` and `f(b)` have different signs, then `f` has a root in `[a, b]`.</b></p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/ivt_root.jpg">
          </div>
          <p>Again, `f` achieves every value between the dotted lines between `a` and `b`. Here, the `x`-axis lies between the dotted lines, so it is guaranteed that `f` will cross the `x`-axis, i.e. `f(c) = 0` at some point `c` between `a` and `b`.</p>
          <p>This suggests that we start the algorithm by picking `a_0, b_0` such that `f(a_0), f(b_0)` have different signs (e.g. `f(a_0)` is positive and `f(b_0)` is negative).</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_0.jpg">
          </div>
          <p>Now we have to reduce the size of the interval. We can split it in half; then, the root has to either be on the left half or the right half. So let's split it at `c_0` (the midpoint) and suppose that `f(c_0)` is positive.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_0.5.jpg">
          </div>
          <p>Now we have two choices for what our new interval `[a_1, b_1]` could be: `[a_0, c_0]` or `[c_0, b_0]`. The IVT doesn't tell us if there's a root between `a_0` and `c_0`, but it does guarantee that there is a root between `c_0` and `b_0`. So the new interval `[a_1, b_1]` should be `[c_0, b_0]`.</p>
          <p>And this process of dividing the interval in half repeats until the interval is small enough.</p>
          <h4>Error of the Bisection Method</h4>
          <p>The thing is, we're looking for a number, but we end up with an interval (two numbers). So how do we get our answer? We could just pick some number between that interval. At this point, the interval is really small, so our approximation should be close to the actual solution. But how much error is there?</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_n.jpg">
          </div>
          <p>Let's pick some number `d_n` and use that as our approximation. In this picture, there would be the most error if the actual solution was at `b_n`. In general,</p>
          <div class="math">
            <p>`abs(text(exact)-text(approximate)) = abs(r - d_n) <= abs(b_n - a_n)`</p>
            <p>where `r` is the exact solution</p>
          </div>
          <div class="box">
            <p>This is because the worst error would occur when the approximation we pick is `a_n` and the actual solution is `b_n` (or vice versa).</p>
          </div>
          <p>We usually pick as our approximation the midpoint of `a_n` and `b_n`. That actually gives us a better approximation because the most error we could have then is half of the previous error. (This would be when the exact solution is either at `a_n` or at `b_n`.)</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/root_algorithm_error.jpg">
            <p>`abs(text(exact)-text(approximate)) = abs(r - c_n) <= abs(b_n - a_n)/2`</p>
          </div>
          <div class="box">
            <p>In general, `abs(r - d_n) <= max(abs(d_n-a_n), abs(d_n-b_n))`.</p>
          </div>
          <p>So to summarize, the algorithm for finding a root of a polynomial is:</p>
          <ol>
            <li><p>Start with an interval `[a_0,b_0]` where `f(a_0)` and `f(b_0)` have different signs</p></li>
            <li><p>Cut the interval in half at `c_0`, which is the midpoint of `[a_0,b_0]`</p></li>
            <li><p>Pick a new (smaller) interval such that either `f(a_0)` and `f(c_0)` have different signs or `f(c_0)` and `f(b_0)` have different signs</p></li>
            <li><p>Repeat the steps for that new (smaller) interval</p></li>
          </ol>
          <p>As the steps suggest, the name of this algorithm is the <b>bisection method</b>.</p>
          <h4>Convergence of the Bisection Method</h4>
          <p>The bisection method generates a sequence of approximations:</p>
          <div class="math">
            <p>`{c_n}_(n=0)^(oo)`</p>
            <p>where `c_n = (a_n + b_n)/2`</p>
          </div>
          <p>If the actual solution is `r`, then what we want to say is that `color(blue)(lim_(n->oo) c_n = r)`. In words, the more times we do the algorithm, the more approximations we will get and the closer those approximations will be to the actual solution `r`.</p>
          <p>Another way to say that is that the difference (i.e. error) between the approximation we get and the actual solution gets closer and closer to `0`. Generally, it might not be guaranteed to happen, but for the bisection method, it is true that the more times we do the algorithm, the closer it will get to `0`.</p>
          <p>Previously, we found that `abs(r - c_n) <= abs(b_n - a_n)/2`. Well that means `abs(r - c_0) <= abs(b_0 - a_0)/2`. And also, `abs(r - c_1) <= abs(b_1 - a_1)/2`.</p>
          <p>But what is `abs(b_1 - a_1)`? Thinking back to the steps of the algorithm, `[a_1,b_1]` represented the new interval after splitting `[a_0,b_0]` in half. So `[a_1,b_1]` is really half of `[a_0,b_0]`. Which means `abs(b_1 - a_1) = abs(b_0 - a_0)/2`.</p>
          <p>Putting all that together, `abs(r - c_1) <= abs(b_1 - a_1)/2 = (abs(b_0 - a_0)/2)/2 = 1/2^2 (b_0 - a_0)`</p>
          <p>Generalizing that idea, we can see that:</p>
          <div class="math">
            <p>`abs(r - c_0) <= 1/2^1 (b_0 - a_0)`</p>
            <p>`abs(r - c_1) <= 1/2^2 (b_0 - a_0)`</p>
            <p>`abs(r - c_2) <= 1/2^3 (b_0 - a_0)`</p>
            <p>`abs(r - c_3) <= 1/2^4 (b_0 - a_0)`</p>
            <p>`vdots`</p>
            <p>`abs(r - c_n) <= 1/2^(n+1) (b_0 - a_0)`</p>
          </div>
          <p>Now, as `n` gets larger and larger (i.e. as we do more steps of the algorithm), the right-hand side gets smaller and smaller. When `n` goes to `oo`, the right-hand side goes to `0`.</p>
          <p>This proves that what we want to say is true: `color(blue)(lim_(n->oo) abs(r - c_n) = 0 => lim_(n->oo)c_n = r)`</p>
          <p>When this happens, we say that the bisection method <em>converges</em>.</p>
          <p>So in theory, the error gets really small when `n` is large. But how large should `n` be? How many times should we keep dividing intervals in half until we get a satisfactory answer?</p>
          <p>If we know that the worst error is `1/2^(n+1) (b_0 - a_0)`, then we can decide how small the error could be.</p>
          <div class="box">
            <p>Note that the error bound is dependent only on `a_0`, `b_0`, and `n`. We can already figure out how much error there will be without even knowing what `f(x)` is.</p>
          </div>
          <div class="box">
            <p>Ex: Let's say the starting interval is `[-1,3]` and that we want the error to be no bigger than `10^(-12)`.</p>
            <div class="math">
              <p>`(3-(-1))/(2^(n+1)) <= 10^(-12)`</p>
              <p>`4/(2^(n+1)) <= 1/10^(12)`</p>
              <p>`4*10^12 <= 2^(n+1)`</p>
              <p>`log_2(4*10^12) <= n+1`</p>
              <p>`n >= log_2(4*10^(12))-1 ~~ 41`</p>
            </div>
            <p>So if we do the bisection method for `41` steps, then the approximation we end up with will have an error of no more than `10^(-12)`.</p>
          </div>
          <div class="box">
            <p>There are other procedures for deciding when to stop without using error bounds.</p>
            <ol>
              <li><p>We could stop when `abs(c_n - c_(n-1))` is small enough. In this case, we would be getting approximations that are unchanging because the error is so small. For example, if we got `{..., 0.98757, 0.98754, 0.98751, 0.98751, 0.98751, 0.98751}`, then we could be pretty sure that `0.98751` is a good approximation.</p></li>
              <li><p>We could stop when `abs(f(c_n))` is small enough. In this case, `abs(f(c_n))` would be really close to `0`, which is what we wanted in the first place (because we're looking for roots).</p></li>
              <li><p>We could stop when the algorithm takes too long or is not converging. This could happen for general algorithms (not the bisection method).</p></li>
            </ol>
          </div>
          <div class="box">
            <p>Matlab code for bisection method</p>
            <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">function</span><span style="color: #bbbbbb"> </span>[c] =<span style="color: #bbbbbb"> </span><span style="color: #0066BB; font-weight: bold">bisection</span>(a,b,n)<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>c = (a<span style="color: #333333">+</span>b)<span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">2</span>;
    <span style="color: #008800; font-weight: bold">for</span> step = <span style="color: #0000DD; font-weight: bold">1</span>:n
        <span style="color: #888888">% f is defined as another function in another .m file</span>
        <span style="color: #008800; font-weight: bold">if</span> f(a)<span style="color: #333333">*</span>f(c) <span style="color: #333333">&lt;</span>= <span style="color: #0000DD; font-weight: bold">0</span>
            b = c;
        <span style="color: #008800; font-weight: bold">else</span>
            a = c;
        <span style="color: #008800; font-weight: bold">end</span>
        c = (a<span style="color: #333333">+</span>b)<span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">2</span>;
     <span style="color: #008800; font-weight: bold">end</span>
<span style="color: #008800; font-weight: bold">end</span>
</pre></div>
          </div>
          <h4>Speed of the Bisection Method</h4>
          <p>Since we're dealing with nonlinear equations, we can say that computing `f(x)` is relatively hard (i.e. time-consuming). Of course, a computer can do it almost instantly, but it takes more time to compute powers and roots than additions and divisions. So we can measure the speed of the bisection method by counting how many times we compute `f(x)`.</p>
          <p>In the code above, we're calcuating `f(a)` and `f(c)` every iteration. That's `2` costly operations. It turns out we can reduce it to only `1` costly operation per iteration if we store their values.</p>
          <div class="box">
            <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">function</span><span style="color: #bbbbbb"> </span>[c] =<span style="color: #bbbbbb"> </span><span style="color: #0066BB; font-weight: bold">bisectionOptimized</span>(a,b,n)<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>c = (a<span style="color: #333333">+</span>b)<span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">2</span>;
    fa = f(a);
    <span style="color: #008800; font-weight: bold">for</span> step = <span style="color: #0000DD; font-weight: bold">1</span>:n
        fc = f(c);
        <span style="color: #008800; font-weight: bold">if</span> fa<span style="color: #333333">*</span>fc <span style="color: #333333">&lt;</span>= <span style="color: #0000DD; font-weight: bold">0</span>
            b = c;
        <span style="color: #008800; font-weight: bold">else</span>
            a = c;
            fa = fc;
        <span style="color: #008800; font-weight: bold">end</span>
        c = (a<span style="color: #333333">+</span>b)<span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">2</span>;
    <span style="color: #008800; font-weight: bold">end</span>
<span style="color: #008800; font-weight: bold">end</span>
</pre></div>
          </div>
          <p>We only need to calculate `f(a)` once at the beginning to use IVT. After that, `a` becomes `c`. At this point, we've also already calculated `f(c)`, so when we're trying to find new intervals, we don't need to calculate `f(c)` again.</p>
          <div class="box">
            <p>Let's look at the bisection method without the code. Let's assume that to find `c_0`, we have to first determine whether `f(a_0)` and `f(b_0)` have opposite signs. Then to find `c_1` we have to compute `f(c_0)` and so on.</p>
            <div class="math">
              <p>`c_0`: `2` total function evaluations so far (`f(a_0)` and `f(b_0)`)</p>
              <p>`c_1`: `3` total function evaluations so far (`f(c_0)` plus previous)</p>
              <p>`c_2`: `4` total function evaluations so far (`f(c_1)` plus previous)</p>
              <p>`vdots`</p>
              <p>`c_n`: `n+2` function evaluations</p>
            </div>
            <p>The `2` function evaluations come from using the IVT to ensure that the root is in between the intervals.</p>
          </div>
          <div class="box">
            <p>Some details regarding the bisection method:</p>
            <ul>
              <li><p>If `f(a)` and `f(c)` are both really large, then it's possible that multiplying them together results in a number that is too large for the computer to store. Then, we would get errors like we saw in the previous section. To avoid this, we could define a function called `sign(f(x)) = {(1 if f(x) > 0),(-1 if f(x) < 0),(0 if f(x) = 0):}`. Then we could check if `sign(f(a)) cancel(=) sign(f(c))` instead of multiplying them.</p></li>
              <li><p>It might sound like a good idea to check if `f(c_n) = 0` to stop the algorithm early. That would mean that in doing our algorithm, we hit on the <em>exact</em> root. The chances of that happening are virtually `0` (imagine marking `sqrt(2)` <em>exactly</em> on a a number line). The bisection method will converge anyway, so checking that is an extra costly operation.</p></li>
              <li><p>Like with the first point, it's possible that when calculating `c = (a+b)/2`, adding `a` and `b` results in a number too large to store. We can avoid this by calculating `c = a + (b-a)/2`. Note that `a + (b-a)/2 = (a + b)/2`.</p></li>
            </ul>
          </div>
          <div class="box">
            <p><b>The story thus far:</b> Now we're looking at how to actually solve problems like `f(x) = g(x)`. One way is to rearrange the equation so that we get `f(x) - g(x) = 0` (for simplicity, we generalize it to `f(x) = 0`). Then the task is to find the roots/zeros of `f(x)`. One way to find a root is to start with a big interval in which the root is guaranteed to lie. Then we repeatedly make the interval smaller and smaller until it's really close to the root.</p>
            <p><b>Speed of bisection method:</b> `1` function evaluation per iteration</p>
            <p><b>Convergence of bisection method:</b> `e_n = 1/(2^(n+1))(b_0 - a_0)`</p>
          </div>
          <h3>Straight to the Point</h3>
          <p>The bisection method is great and all, but are there other ways that might be faster?</p>
          <p>Another method we could use is Newton's method. In this method, we use tangent lines to approximate the root.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/newton.jpg">
          </div>
          <p>The idea is to start at an initial guess `x_0` and repeatedly draw tangent lines until we (hopefully) get close to the root.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/comic_newton.jpg">
          </div>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/newton_iterative.jpg">
          </div>
          <p>Using the point-slope formula for the equation of a line, we can get the equation of the tangent line at `x_0`:</p>
          <div class="math">
            <p>`y - f(x_0) = f'(x_0)(x - x_0)`</p>
            <p>`y = f'(x_0)(x - x_0) + f(x_0)`</p>
          </div>
          <p>Our goal is to get `y = 0`, so we have:</p>
          <div class="math">
            <p>`0 = f'(x_0)(x - x_0) + f(x_0)`</p>
          </div>
          <p>Solving for `x`, we get:</p>
          <div class="math">
            <p>`x = x_0 - f(x_0)/(f'(x_0))`</p>
          </div>
          <p>This is the intuition behind the iterative formula for Newton's method:</p>
          <div class="math">
            <p>`x_(n+1) = x_n - f(x_n)/(f'(x_n))`</p>
          </div>
          <div class="box">
            <p>There are some cases where Newton's method won't work.</p>
            <div class="math">
              <img class="img-fluid" src="../pictures/nonlinear/newton_farther.jpg">
              <p>Here, each approximation results in a tangent line that just keeps going to the right.</p>
              <img class="img-fluid" src="../pictures/nonlinear/newton_bounce.jpg">
              <p>Here, the approximations will just keep bouncing back and forth between `x_0` and `x_1`.</p>
              <img class="img-fluid" src="../pictures/nonlinear/newton_parallel.jpg">
              <p>Here, the tangent line doesn't cross the `x`-axis, so we can't get an approximation.</p>
            </div>
            <p>So unlike the bisection method, Newton's method won't always work. However, it's still worth looking at Newton's method, because it turns out that it is generally faster than the bisection method.</p>
          </div>
          <h4>Error of Newton's Method</h4>
          <div class="box">
            <p>A Taylor series represents a function as an infinite sum.</p>
            <div class="math">
              <p>`f(x) = f(a) + (f'(a))/(1!)(x-a) + (f''(a))/(2!)(x-a)^2 + (f'''(a))/(3!)(x-a)^3 + ...`</p>
              <p>`f(x) = sum_(n=0)^(oo) (f^(n)(a))/(n!)(x-a)^(n)`</p>
            </div>
          </div>
          <p>Notice that the first `2` terms of the Taylor series are the equation of a line for `f(x)` (and thus, are a linear approximation of `f(x)`). Since we're looking at approximations using lines, we'll consider the first `2` terms and collect the rest of the terms as some error.</p>
          <div class="math">
            <p>`f(x) = f(x_n) + f'(x_n)(x-x_n) + (f''(xi_n))/2(x-x_n)^2`</p>
            <p>for some `xi_n` between `x` and `x_n`</p>
          </div>
          <p>Since we're interested in finding the root of `f(x)`, we can use `x = r` where `r` is a root to get:</p>
          <div class="math">
            <p>`0 = f(x_n) + f'(x_n)(r-x_n) + (f''(xi_n))/2(r-x_n)^2`</p>
            <p>`r = x_n - f(x_n)/(f'(x_n)) - (f''(xi_n))/(2f'(x_n))(r-x_n)^2`</p>
          </div>
          <p>Well, `x_n - f(x_n)/(f'(x_n)) = x_(n+1)` from the iterative formula for Newton's method, so we have:</p>
          <div class="math">
            <p>`r = x_(n+1) - (f''(xi_n))/(2f'(x_n))(r-x_n)^2`</p>
            <p>`r - x_(n+1) = (f''(xi_n))/(2f'(x_n))(r-x_n)^2`</p>
          </div>
          <p>Then, we can let `e_n = x_n - r` be the error between the approximation and the root (so `e_(n+1) = x_(n+1)-r`).</p>
          <div class="math">
            <p>`e_(n+1) = (f''(xi_n))/(2f'(x_n))(e_n)^2`</p>
          </div>
          <p><b>This means that the error between our `(n+1)^(st)` approximation and the root is roughly equal to the square of the error between the `n^(th)` approximation and the root. So if the error was small for the `n^(th)` approximation, the error will be even smaller for the `(n+1)^(st)` approximation.</b></p>
          <div class="box">
            <ul>
              <li><p>If `f'(x_n) = 0` (the slope of the tangent line is horizontal), then we get a `0` in the denominator. This explains one of the cases where Newton's method doesn't work mentioned earlier.</p></li>
              <li><p>If `f''(xi_n) = oo` (the second derivative of `f` doesn't exist), then we get really large error.</p>
            </ul>
          </div>
          <h4>Convergence of Newton's Method</h4>
          <p>Let's suppose `f'`,`f''` are continuous around `x = r` and `f'(r) != 0` to avoid the problems mentioned above. Then if we start with an intial guess close enough to `r`, we can get `x_n -> r` (the `n^(th)` approximation will approach `r`).</p>
          <div class="box">
            <p>Let's suppose we are close enough so that `(f''(xi_n))/(2f'(x_n)) ~~ (f''(r))/(2f'(r))`. Then `e_(n+1) ~~ (f''(r))/(2f'(r))(e_n)^2 = ((f''(r))/(2f'(r))e_n)e_n`.</p>
            <p>If `((f''(r))/(2f'(r))e_n) < 1`, then `e_(n+1) -> 0`.</p>
          </div>
          <p>Since `(f''(r))/(2f'(r))` is a constant, we can call it `c`. Then the error for Newton's method can be represented as `e_(n+1) ~~ c(e_n)^2`. Let's say the error of the `n^(th)` approximation was `0.001`.</p>
          <div class="flex">
            <table>
              <tr>
                <td></td>
                <td>Bisection Method</td>
                <td>Newton's Method</td>
              </tr>
              <tr>
                <td>Error Formula</td>
                <td>`e_(n+1) = 1/2e_n`</td>
                <td>`e_(n+1) ~~ c(e_n)^2`</td>
              </tr>
              <tr>
                <td>`e_n`</td>
                <td>`0.001`</td>
                <td>`0.001`</td>
              </tr>
              <tr>
                <td>`e_(n+1)`</td>
                <td>`0.0005`</td>
                <td>`c*0.000001`</td>
              </tr>
            </table>
          </div>
          <p>From this, we can see that Newton's method (generally) converges faster than the bisection method. The convergence of Newton's method is described as "quadratic convergence" because of the power `2`.</p>
          <h4>Speed of Newton's Method</h4>
          <p>The iterative formula for Newton's method is `x_(n+1) = x_n - f(x_n)/(f'(x_n))`. From this, we can see that it requires `2` function evaluations per iteration. The bisection method required `2` function evaluations at the beginning, but then it only required `1` function evaluation per iteration. <b>This means that Newton's method does roughly twice as many function evaluations per iteration as the bisection method.</b> This also means that Newton's method is <em>slower</em> than the bisection method, but this is offset by the fact that it converges faster.</p>
          <h4>Convergence of Newton's Method, Revisited</h4>
          <p>So one convergence result we have is that if `f''` is continuous near `r` and `f'(r) != 0`, then choosing an `x_0` sufficiently close to `r` will result in convergence of Newton's method.</p>
          <p>Another convergence result is that if `f'' > 0` (`f` is concave up) in the interval `[r, oo)` and `f' > 0` (`f` is increasing) in the interval `[r,oo)`, then `x_0 in [r,oo)` implies Newton's method will converge.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/concaveup_increasing.jpg">
          </div>
          <p>This is an example where `f'' > 0` and `f' > 0`.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/concaveup_increasing_convergence.jpg">
          </div>
          <p>We can see that if we continue, we will eventually hit `r`. This is what the second convergence result is saying. It's important to note that we have to start at an `x_0` to the right of `r`, i.e. `x_0 > r`.</p>
          <div class="box">
            <p>To prove the second convergence result, we have to show that:</p>
            <ol>
              <li><p>`x_n` is decreasing, i.e. `x_n < ... < x_3 < x_2 < x_1 < x_0`</p></li>
              <li><p>a decreasing sequence bounded from below has to converge</p></li>
              <li><p>what it converges to is `r`</p>
            </ol>
          </div>
          <p>Before, we prove those `3` points, it's useful to first prove `2` lemmas.</p>
          <div class="box">
            <p>Lemma: If `{x_n}_(n=0)^(oo)` is the sequence of approximations obtained from Newton's method, then `x_n in [r,oo)`. In words, every approximation will be to the right of `r`.</p>
            <div class="collapse" id="collapseNewtonLemma1">
              <p>We have `x_(n+1) - r = e_(n+1) = (f''(xi_n))/(2f'(x_n))(e_n)^2`</p>
              <p>For the approximations before reaching `r`, there will be some error, i.e. `e_n >= 0`</p>
              <p>If `e_n >= 0`, then `e_(n+1) = (f''(xi_n))/(2f'(x_n))(e_n)^2 >= 0`. This is because we assumed `f'' > 0` & `f' > 0`, and `e_n >= 0 => (e_n)^2 >= 0`</p>
              <p>So we have `e_(n+1) = x_(n+1) - r >= 0 => x_(n+1) >= r`</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseNewtonLemma1">Proof</a>
          </div>
          <div class="box">
            <p>Lemma: Suppose `x_n -> s`. Then `s` is a root of `f`. In other words, if we keep taking approximations and they all start to equal a certain number, then that number must be the root.</p>
            <div class="collapse" id="collapseNewtonLemma2">
              <p>We have `x_(n+1) = x_n - f(x_n)/(f'(x_n))`</p>
              <p>Since `f` and `f'` are continuous, `lim_(n->oo) x_(n+1) = lim_(n->oo) x_n - f(x_n)/(f'(x_n))` (we can take limits of both sides because they're continuous functions)</p>
              <p>Since we have `x_n -> s`, `lim_(n->oo) x_(n+1) = s` and `lim_(n->oo) x_n = s`</p>
              <p>So we end up with `s = s - f(s)/(f'(s))`</p>
              <p>`=> 0 = -f(s)/(f'(s)) => f(s) = 0`, which means `s` is a root</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseNewtonLemma2">Proof</a>
          </div>
          <div class="box">
            <p>1. `x_n` is decreasing</p>
            <div class="collapse" id="collapseNewton1">
              <p>We have `x_(n+1) = x_n - f(x_n)/(f'(x_n))`</p>
              <p>Since `f' > 0`, `f(r) = 0`, and `x_n > r`, `f(x_n) >= 0`</p>
              <p>Also, since `f' > 0`, `f'(x_n) > 0`</p>
              <p>So `f(x_n)/(f'(x_n)) >= 0`</p>
              <p>This means that `x_(n+1)` is obtained by getting `x_n` and taking away a part of it. If you take away a part of the original amount, then the amount you end up with will always be less than the original amount.</p>
              <p>So `x_(n+1) < x_n`</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseNewton1">Proof</a>
          </div>
          <div class="box">
            <p>2. a decreasing sequence bounded from below has to converge</p>
            <div class="collapse" id="collapseNewton2">
              <p>If every approximation is to the right of `r`, then we will never go below (to the left of) `r` (from first lemma)</p>
              <p>But we also can't be jumping randomly either because `x_n` is decreasing (from 1.)</p>
              <p>So we have to keep moving left but never past `r`</p>
              <p>Eventually, we will hit a point where we can't keep moving left because we have to stay to the right of `r`</p>
              <p>Note: at this point, where it stops doesn't necessarily have to be at `r`</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseNewton2">Proof</a>
          </div>
          <div class="box">
            <p>3. what it converges to is `r`</p>
            <div class="collapse" id="collapseNewton3">
              <p>Using the point-slope formula, we get the equation of the line passing through `s` and `r`: `f(s) = f(r) + f'(alpha_n)color(red)(text(*))(s-r)` for some `alpha_n` between `s` and `r`</p>
              <p>From the second lemma, `s` is a root, so `f(s) = 0`. Also, `f(r)` is a root by definition</p>
              <p>`0 = 0 = f'(alpha_n)(s-r)`</p>
              <p>`f'(alpha_n) != 0` since `f' > 0`, so we can divide both sides by `f'(alpha_n)`</p>
              <p>`s - r = 0 => s = r`</p>
              <p>`color(red)(text(*))``f'(alpha_n)` is the slope of the secant line passing through `s` and `r`</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseNewton3">Proof</a>
          </div>
          <p>Note: we proved that Newton's method converges on the interval `[r,oo)` when `f'' > 0` and `f' > 0`. We can take a similar approach to prove that Newtons' method converges:</p>
          <ul>
            <li><p>on the interval `[r,oo)` when `f'' < 0` and `f' < 0`</p></li>
            <li><p>on the interval `(-oo,r]` when `f'' > 0` and `f' < 0`</p></li>
            <li><p>on the interval `(-oo,r]` when `f'' < 0` and `f' > 0`</p></li>
          </ul>
          <div class="box">
            <p><b>The story thus far:</b> Newton's method approximates a root by repeatedly drawing tangent lines until the root of the tangent line matches the root of the function. It has to do more function evaluations per iteration than the bisection method, but it converges faster than the bisection method.</p>
            <p><b>Speed of Newton's method:</b> `2` function evaluations per iteration</p>
            <p><b>Convergence of Newton's method:</b> `e_(n+1) = (f''(xi_n))/(2f'(x_n))(e_n)^2` for `xi_n` between `x_n` and `r`</p>
          </div>
          <h3>Oh See Can't You See</h3>
          <p>So Newton's method converges faster than the bisection method does, but it requires more function evaluations. Is there a method that converges faster than the bisection method and requires less function than Newton's method?</p>
          <p>Newton's method looked at tangent lines. Let's look instead at secant lines.</p>
          <p>The secant method is a variation of Newton's method. It still uses the iterative formula `x_(n+1) = x_n - f(x_n)/(f'(x_n))`, but the difference is that `f'(x_n)` will be the slope of the secant line, not the tangent line.</p>
          <p>The derivative can be calculated by `f'(a) = lim_(x->a) (f(x)-f(a))/(x-a)`. Without the limit, it can be approximated by `f'(a) ~~ (f(x)-f(a))/(x-a)`.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/secant.jpg">
          </div>
          <p>One way to get `f'(x_n)` is to consider the slope of the secant line through `x_(n-1)` and `x_n`. Then we have `f'(x_n) ~~ (f(x_n)-f(x_(n-1)))/(x_n-x_(n-1))`. This leads to the iterative formula for the secant method:</p>
          <div class="math">
            <p>`x_(n+1) = x_n - f(x_n)/((f(x_n)-f(x_(n-1)))/(x_n-x_(n-1)))`</p>
            <p>`x_(n+1) = x_n - (f(x_n)(x_n - x_(n-1)))/(f(x_n)-f(x_(n-1)))`</p>
          </div>
          <p>The idea is that we're getting our next approximation by drawing the secant line through our previous two approximations.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/nonlinear/secant_method.jpg">
          </div>
          <p>Notice how using the previous two approximations means we only need `1` function evaluation per iteration.</p>
          <div class="flex">
            <table>
              <tr>
                <td></td>
                <td>all function evaluations needed</td>
                <td>new function evaluations needed</td>
              </tr>
              <tr>
                <td>`x_0`</td>
                <td>`f(x_0)`</td>
                <td>`f(x_0)`</td>
              </tr>
              <tr>
                <td>`x_1`</td>
                <td>`f(x_1)`</td>
                <td>`f(x_1)`</td>
              </tr>
              <tr>
                <td>`x_2`</td>
                <td>`f(x_0), f(x_1)`</td>
                <td>none</td>
              </tr>
              <tr>
                <td>`x_3`</td>
                <td>`f(x_1), f(x_2)`</td>
                <td>`f(x_2)`</td>
              </tr>
              <tr>
                <td>`x_4`</td>
                <td>`f(x_2), f(x_3)`</td>
                <td>`f(x_3)`</td>
              </tr>
            </table>
          </div>
          <h4>Error of Secant Method</h4>
          <p>To look at the error, we can start by rewriting the iterative formula:</p>
          <div class="math">
            <p>`x_(n+1) = x_n - (f(x_n)(x_n - x_(n-1)))/(f(x_n)-f(x_(n-1)))`</p>
            <p>`x_(n+1) = (x_n(f(x_n)-f(x_(n-1))))/(f(x_n)-f(x_(n-1))) - (x_nf(x_n) - x_(n-1)f(x_n))/(f(x_n)-f(x_(n-1)))`</p>
            <p>`x_(n+1) = (x_nf(x_n) - x_nf(x_(n-1)) - x_nf(x_n) + x_(n-1)f(x_n))/(f(x_n)-f(x_(n-1)))`</p>
            <p>`x_(n+1) = (x_(n-1)f(x_n) - x_nf(x_(n-1)))/(f(x_n)-f(x_(n-1)))`</p>
          </div>
          <p>Now we can look at `e_(n+1) = x_(n+1) - r`:</p>
          <div class="math">
            <p>`x_(n+1)-r = (x_(n-1)f(x_n) - x_nf(x_(n-1)))/(f(x_n)-f(x_(n-1)))-r`</p>
            <p>`x_(n+1)-r = (x_(n-1)f(x_n) - x_nf(x_(n-1)))/(f(x_n)-f(x_(n-1)))-(r(f(x_n)-f(x_(n-1))))/(f(x_n)-f(x_(n-1)))`</p>
            <p>`x_(n+1)-r = (x_(n-1)f(x_n) - x_nf(x_(n-1)) - rf(x_n) + rf(x_(n-1)))/(f(x_n)-f(x_(n-1)))`</p>
          </div>
          <p>Using the fact that `e_(n-1) = x_(n-1)-r` and `e_n = x_n - r`:</p>
          <div class="math">
            <p>`x_(n+1)-r = (e_(n-1)f(x_n)-e_nf(x_(n-1)))/(f(x_n)-f(x_(n-1)))`</p>
            <p>`x_(n+1)-r = (f(x_n)/e_n - f(x_(n-1))/e_(n-1))/(f(x_n)-f(x_(n-1)))(e_(n-1)e_n)`</p>
            <p>by factoring out `(e_(n-1)e_n)`</p>
          </div>
          <p>Multiplying by `1 = (x_n-x_(n-1))/(x_n-x_(n-1))`:</p>
          <div class="math">
            <p>`x_(n+1)-r = ((x_n-x_(n-1))/(f(x_n)-f(x_(n-1)))*(f(x_n)/e_n-f(x_(n-1))/e_(n-1))/(x_n-x_(n-1)))e_(n-1)e_n`</p>
          </div>
          <div class="box">
            <p>`(x_n-x_(n-1))/(f(x_n)-f(x_(n-1)))` is a constant</p>
            <div class="collapse" id="collapseSecant1">
              <p>Taylor series of `f` centered at `r`:</p>
              <p>`f(x_n) = f(r) + f'(r)(x_n-r) + (f''(r))/2(x_n-r)^2 + ... = f(r) + f'(r)e_n + (f''(r))/2(e_n)^2 + ...`</p>
              <p>Similarly, `f(x_(n-1)) = f(r) + f'(r)e_(n-1) + (f''(r))/2(e_(n-1))^2 + ...`</p>
              <p>Subtracting `f(x_(n-1))` and `f(x_n)` gives us:</p>
              <p>`f(x_n)-f(x_(n-1)) = f'(r)(e_n-e_(n-1)) + ...`</p>
              <p>Well, `e_n-e_(n-1) = (x_n-r)-(x_(n-1)-r)=x_n-x_(n-1)`</p>
              <p>So `f(x_n)-f(x_(n-1)) ~~ f'(r)(x_n-x_(n-1))`</p>
              <p>Which means `(x_n-x_(n-1))/(f(x_n)-f(x_(n-1))) ~~ (x_n-x_(n-1))/(f'(r)(x_n-x_(n-1))) = 1/(f'(r))`, which is a constant</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseSecant1">Proof</a>
          </div>
          <div class="box">
            <p>`(f(x_n)/e_n-f(x_(n-1))/e_(n-1))/(x_n-x_(n-1))` is a constant</p>
            <div class="collapse" id="collapseSecant2">
              <p>`f(x_n)/e_n = f'(r) + (f''(r))/2e_n + ...`</p>
              <p>`f(x_(n-1))/e_(n-1) = f'(r) + (f''(r))/2e_(n-1) + ...`</p>
              <p>Subtracting `f(x_(n-1))/e_(n-1)` and `f(x_n)/e_n` gives us:</p>
              <p>`f(x_n)/e_n-f(x_(n-1))/e_(n-1) = (f''(r))/2(e_n-e_(n-1)) + ...`</p>
              <p>Well, `e_n-e_(n-1) = (x_n-r)-(x_(n-1)-r)=x_n-x_(n-1)`</p>
              <p>So `f(x_n)/e_n-f(x_(n-1))/e_(n-1) ~~ (f''(r))/2(x_n-x_(n-1))`</p>
              <p>Which means `(f(x_n)/e_n-f(x_(n-1))/e_(n-1))/(x_n-x_(n-1)) ~~ ((f''(r))/2(x_n-x_(n-1)))/(x_n-x_(n-1)) = (f''(r))/2`, which is a constant</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseSecant2">Proof</a>
          </div>
          <p>Combining the results above, we get that `x_(n+1)-r = e_(n+1) ~~ Ce_(n-1)e_n` where `C = (f''(r))/(2f'(r))`. If we want to compare this error to the error in Newton's method (which was `e_(n+1) = c(e_n)^2`), then we should rewrite `e_(n+1) = Ce_(n-1)e_n` in the form of `A(e_n)^(alpha)` and find out what `alpha` is. If `alpha < 2`, then the secant method converges slower than Newton's method. If `alpha > 2`, then the secant method converges faster than Newton's method.</p>
          <div class="box">
            <p>It turns out that `alpha ~~ 1.62`</p>
            <div class="collapse" id="collapseSecant3">
              <p>Suppose `e_(n+1) = Ce_(n-1)e_n = A(e_n)^(alpha)`</p>
              <p>`e_n = A(e_(n-1))^(alpha)`</p>
              <p>`e_(n-1) = (e_n)^(1/alpha)/A^(1/alpha)`</p>
              <p>So `Ce_(n-1)e_n = C(e_n)^(1/alpha)/A^(1/alpha)A(e_(n-1))^(alpha)`</p>
              <p>`e_n = A(e_(n-1))^(alpha) => (e_(n-1))^(alpha) = e_n/A`</p>
              <p>`Ce_(n-1)e_n = C(e_n)^(1/alpha)/A^(1/alpha)A(e_n/A) = C(e_n)^(1/alpha)/A^(1/alpha)e_n = C/A^(1/alpha)(e_n)^(1+1/alpha)`</p>
              <p>So `A(e_n)^(alpha) = C/A^(1/alpha)(e_n)^(1+1/alpha)`, which means `alpha = 1 + 1/alpha`</p>
              <p>`alpha = 1 + 1/alpha => alpha^2 = alpha + 1 => alpha^2-alpha-1 = 0`</p>
              <p>`alpha = (1 +- sqrt(5))/2 ~~ 1.62`</p>
            </div>
            <a class="btn" data-toggle="collapse" href="#collapseSecant3">Proof</a>
          </div>
          <p>So `x_(n+1)-r = e_(n+1) = A(e_n)^(1.62)`, which means the secant method converges slower than Newton's method.</p>
        </div> <!-- col -->
      </div> <!-- row -->
    </div> <!-- container -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
