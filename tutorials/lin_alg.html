<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
    -->
    <title>Numerical Analysis: A Linear Algebra Story</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="lin_alg.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <a href="tutorials.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
          <h1>Numerical Analysis: A Linear Algebra Story</h1>
          <p>Linear algebra is all about solving `Ax = b,` where, usually, `A` is an `n xx n` matrix, `x` is an `n xx 1` matrix, and `b` is an `n xx 1` matrix. Specifically, given `A` and `b,` find `x`. In other words, if the problem is</p>
          <div class="math">
            <p>`[[a_(1,1), a_(1,2), ..., a_(1,n)],[a_(2,1), a_(2,2), ..., a_(2,n)],[..., ..., ..., ...],[a_(n,1), a_(n,2), ..., a_(n,n)]] [[x_1], [x_2], [...], [x_n]] = [[b_1], [b_2], [...], [b_n]]`</p>
          </div>
          <p>where we know all the `a` and `b` values, what values of `x` make that true?</p>
          <p>Concretely, what values of `x_1, x_2,` and `x_3` make that equation true?</p>
          <div class="math">
            <p>`[[1, 4, 1],[8, 2, 0],[5, 6, 2]][[x_1],[x_2],[x_3]] = [[24],[32],[49]]`</p>
          </div>
          <p>Of course, we could try plugging in random values for x and hope that they work. But that would take forever. Even if we did it with a computer, it would be impractical to do so all the time.</p>
          <h2>Finding `A^-1`</h2>
          <p>A <i>slightly</i> better approach would be to "isolate" the `x`. As from elementary algebra, we could solve for `x` in `5x = 10` by dividing both sides by `5`. So, for `Ax = b`, why not "divide" both sides by `A`? (More formally known as taking the inverse). There are some special cases in which it is impossible to take the inverse of `A`, so for simplicity, we avoid those cases and assume it is possible, i.e., `A` is "invertible".</p>
          <p>In theory, the answer really is that simple. Multiply both sides (on the left) by the inverse of `A` (denoted by `A^-1`) to get `x = A^-1b`. Done. In practice, though, this process is actually pretty complicated and time-consuming, <i>especially</i> when done by hand. In fact, doing it by hand is so nightmarish, let's assume we have a computer program that inverts and multiplies matrices for us. It turns out that even with a computer doing millions of operations in mere milliseconds, <b>inverting a matrix and then multiplying is relatively time-consuming</b>.</p>
          <p>It's natural to assume that if a program needs to do a lot of operations `(+,-,xx,//,sqrt())`, it will take longer than one that needs to do less operations. So <b>we'll consider how long inverting a matrix takes by looking at how many operations it takes.</b> The process is complicated (for me to know/explain), but it turns out that it generally takes `n^3` operations. This is quite a lot of operations. It also means that if you double the size of the matrix, you have to do `8` times more operations to find its inverse. (`2 xx 2` matrix: `8` operations, `4 xx 4` matrix: `64` operations) Most matrices in real life are really large. (`1,000,000 xx 1,000,000` is likely a conservative estimate. Imagine `1,000,000^3` operations.)</p>
          <div class="box">
            <p><b>The story thus far</b>: We're trying to solve `Ax = b`. (Given `A` and `b`, find `x`.) In theory, the simplest approach is to multiply both sides by `A^-1` to get `x = A^-1b`. In practice, this approach is not simple at all. Compared to other methods of solving this problem, finding `A^-1` is time-consuming.</p>
            <p><b>Time complexity of finding `A^-1`</b>: `n^3`</p>
          </div>
          <h2>Triangular Systems</h2>
          <p>Let's assume `A` can be "split up" into two matrices: one lower-triangular matrix (denoted by `L`) and one upper-triangular matrix (denoted by `U`). So `A = LU:`</p>
          <div class="math">
            <p>`[[a_(1,1), a_(1,2), ..., a_(1,n)],[a_(2,1), a_(2,2), ..., a_(2,n)],[..., ..., ..., ...],[a_(n,1), a_(n,2), ..., a_(n,n)]] = [[l_(1,1), 0, ..., 0],[l_(2,1), l_(2,2), 0, ...],[..., ..., ..., 0],[l_(n,1), l_(n,2), ..., l_(n,n)]] [[u_(1,1), u_(1,2), ..., u_(1,n)],[0, u_(2,2), ..., u_(2,n)],[..., 0, ..., ...],[0, ..., 0, u_(n,n)]]`.</p>
          </div>
          <p class="box">Note: Lower-triangular means `0`'s on top right and numbers on bottom left. Upper-triangular means numbers on top right and `0`'s on bottom left. See example below.</p>
          <p>For example,</p>
          <div class="math">
            <p>`[[6, 4, 2, 2],[-3, 0, 3, 5],[9, 7, 7, 5],[12, 9, 12, 16]] = [[2, 0, 0, 0],[-1, 2, 0, 0],[3, 1, -1, 0],[4, 1, -3, 3]] [[3, 2, 1, 1],[0, 1, 2, 3],[0, 0, -2, 1],[0, 0, 0, 4]]`</p>
          </div>
          <p>The problem now becomes `(A)x = b => (LU)x = b.` Where do go from here? The answer lies in the special property of lower- and upper-triangular matrices. (Namely, the zeros.)</p>
          <p>First of all, it is quite easy to solve something like `Lx = b`. For example,</p>
          <div class="math">
            <p>`[[color(red)(5), color(red)(0), color(red)(0)],[color(blue)(2), color(blue)(-4), color(blue)(0)],[color(green)(1), color(green)(2), color(green)(3)]] [[x_1],[x_2],[x_3]] = [[color(red)(15)],[color(blue)(-2)],[color(green)(10)]]`</p>
            <p>`color(red)(5)x_1 + color(red)(0)x_2 + color(red)(0)x_3 = color(red)(15) => x_1 = 3`</p>
            <p>`color(blue)(2)x_1 + color(blue)(-4)x_2 + color(blue)(0)x_3 = color(blue)(-2) => x_2 = 2` (plugging in `3` for `x_1`)</p>
            <p>`color(green)(1)x_1 + color(green)(2)x_2 + color(green)(3)x_3 = color(green)(10) => x_3 = 1` (plugging in `3` for `x_1` and `2` for `x_2`)</p>
            <p>(<b>This process is known as forward substitution.</b>)</p>
          </div>
          <p>We have `LUx = b`. To take advantage of the easiness of forward substitution, let's view the problem as: `L(y) = b`, where `y = Ux`. Solving `Ly = b` like in the previous example, we easily obtain values for `y`. But how do we get `x`?</p>
          <p>It's also just as easy to solve something like `Ux = b`. For example,</p>
          <div class="math">
            <p>`[[color(green)(3), color(green)(2), color(green)(4)],[color(blue)(0), color(blue)(1), color(blue)(2)],[color(red)(0), color(red)(0), color(red)(3)]] [[x_1],[x_2],[x_3]] = [[color(green)(19)],[color(blue)(8)],[color(red)(9)]]`</p>
            <p>`color(red)(0)x_1 + color(red)(0)x_2 + color(red)(3)x_3 = color(red)(9) => x_3 = 3`</p>
            <p>`color(blue)(0)x_1 + color(blue)(1)x_2 + color(blue)(2)x_3 = color(blue)(8) => x_2 = 2` (plugging in `3` for `x_3`)</p>
            <p>`color(green)(3)x_1 + color(green)(2)x_2 + color(green)(4)x_3 = color(green)(19) => x_1 = 1` (plugging in `3` for `x_3` and `2` for `x_2`)</p>
            <p>(<b>This process is known as back substitution.</b>)</p>
          </div>
          <p>Notice before, we defined `y = Ux`. We have `y` (from solving `Ly = b`), and we have `U` (from assuming it was possible to split `A = LU`). So like in the previous example, it is possible to solve for `x`. Let's see how this plays out with real numbers.</p>
          <p>Let `A = [[2, 1],[6, 5]]`, `b = [[4],[16]]`. Our goal is to find `x_1, x_2` such that `[[2, 1],[6, 5]] [[x_1],[x_2]] = [[4],[16]]`</p>
          <p>Let's also say we knew exactly how to split up `A`:</p>
          <div class="math">
            <p>`A = LU = [[1, 0],[3, 2]] [[2, 1],[0, 1]]`</p>
          </div>
          <p>The new problem becomes:</p>
          <div class="math">
            <p>`[[1, 0],[3, 2]] [[2, 1],[0, 1]] [[x_1],[x_2]] = [[4],[16]]`</p>
            <p>(1)</p>
          </div>
          <p>Let's let</p>
          <div class="math">
            <p>`[[y_1],[y_2]] = [[2, 1],[0, 1]] [[x_1],[x_2]]`</p>
            <p>(2)</p>
          </div>
          <p>so that</p>
          <div class="math">
            <p>`[[1, 0],[3, 2]] [[y_1],[y_2]] = [[4],[16]]`</p>
            <p>`1y_1 + 0y_2 = 4 => y_1 = 4`</p>
            <p>`3y_1 + 2y_2 = 16 => y_2 = 2`</p>
            <p>(3)</p>
          </div>
          <p>Now that we have our values for `y`, let's go back to (2):</p>
          <div class="math">
            <p>`[[4],[2]] = [[2, 1],[0, 1]] [[x_1],[x_2]]`</p>
            <p>`0x_1 + 1x_2 = 2 => x_2 = 2`</p>
            <p>`2x_1 + 1x_2 = 4 => x_1 = 1`</p>
            <p>(4)</p>
          </div>
          <p>And we have our values for x, as desired.</p>
          <p class="box">To summarize: We assumed `A` could be "split up" into two parts `L` and `U` (i.e., `A = LU`). With that, the problem became `Ax = b => LUx = b`. We took advantage of the fact that solving something like `Lx = b` was easy. So by letting `y = Ux`, we could solve `Ly = b`. Doing that gave us `y`, so we could get `x` by solving `Ux = y`.</p>
          <p>This process may look like it requires a lot of steps. <b>But it actually takes less operations than inverting a matrix.</b> Let's look at how many operations it takes to solve `Ly = b`. <b>We'll count operations by counting how many numbers we have to perform operations on.</b></p>
          <p>For the first row, there is `1` number. (By the nature of matrix multiplication, the `0`'s don't count as numbers.)</p>
          <div class="math">
            <p>`[[color(red)(5), 0, 0],[2, -4, 0],[1, 2, 3]] [[color(red)(x_1)],[x_2],[x_3]] = [[15],[-2],[10]]`</p>
          </div>
          <p>For the second row, there are `2` numbers.</p>
          <div class="math">
            <p>`[[5, 0, 0],[color(red)(2), color(red)(-4), 0],[1, 2, 3]] [[color(red)(x_1)],[color(red)(x_2)],[x_3]] = [[15],[-2],[10]]`</p>
          </div>
          <p>This continues until the `n^(th)` row where there are `n` numbers.</p>
          <div class="math">
            <p>`[[5, 0, 0],[2, -4, 0],[color(red)(1), color(red)(2), color(red)(3)]] [[color(red)(x_1)],[color(red)(x_2)],[color(red)(x_3)]] = [[15],[-2],[10]]`</p>
          </div>
          <p>So there are `1 + 2 + ... + n ~~ n^2` total numbers* we have to perform operations on.</p>
          <p>*Per Big O notation, constants and lower power terms are dropped. It's actually `(n(n+1))/2 = (n^2 + n)/2` numbers.</p>
          <div class="box">
            <p>A picture for the above statement:</p>
            <div class="math">
              <p>`[[***,*,*,*,*],[***,***,*,*,*],[***,***,***,*,*],[***,***,***,***,*],[***,***,***,***,***]]`</p>
            </div>
            <p>Counting the number of stars, notice that it takes up about <i>half</i> of the area of the square. Hence, about `n^2/2` numbers.</p>
          </div>
          <p><b>This means we have to do <i>about</i> `n^2` operations to solve `Ly = b` (forward substitution).</b> Now we have to solve `Ux = y` (back substitution). By similar reasoning, `U` also has `n^2/2` numbers, so it also takes <i>about</i> `n^2` operations. <b>Altogether, the total number of operations to solve `LUx = b` is `n^2 + n^2 = 2n^2.`</b> For simplicity, we can consider it as `n^2` operations.</p>
          <p>This is <i>definitely</i> less than the `n^3` operations it took to find `A^-1.`</p>
          <div class="box">
            <p><b>The story thus far</b>: Finding `A^-1` simply takes too long `(n^3).` We found out that if we could split `A` into two parts, `L` and `U`, it would only take about `n^2` operations to solve for `x`.
            <p><b>Time complexity for forward and back substitution</b>: `n^2`</p>
          </div>
          <h2>Cholesky Decomposition</h2>
          <p>It turns out that it generally isn't easy finding matrices `L` and `U` such that `A = LU.` But splitting `A` into triangular matrices made solving `Ax = b` so fast though. Relatively fast anyway. Is it possible to still use this idea of splitting matrices into `2` parts, but with the parts (relatively) easy to find?</p>
          <p>It is, but we're gonna have to assume more things about the matrix `A`. Let's assume that there exists an upper-triangular matrix `R` such that `A = R^TR.` <b>So now, instead of finding two matrices `L` and `U` such that `A = LU,` we only need to find `1` matrix `R` such that `A = R^TR.` (`R` is called the "Cholesky factor". This process is called "Cholesky decomposition".)</b> Since `R` is upper-triangular, `R^T` will be lower-triangular, so we can still apply the forward and back substitution methods as before.</p>
          <p class="box">Note: `T` means matrix transpose.</p>
          <p>For example, `A = [[1, 1, 1],[1, 2, 2],[1, 2, 3]]` can be split up into:</p>
          <div class="math">
            <p>`[[1, 1, 1],[1, 2, 2],[1, 2, 3]] = [[1, 0, 0],[1, 1, 0],[1, 1, 1]] [[1, 1, 1],[0, 1, 1],[0, 0, 1]]`</p>
          </div>
          <p>where `R = [[1, 1, 1],[0, 1, 1],[0, 0, 1]]`, and `R^T = [[1, 0, 0],[1, 1, 0],[1, 1, 1]]`</p>
          <p>I know we made a lot of assumptions about `A`, so all of this looks like it'll only work for super-specific cases. However, it turns out that most of the useful matrices used in real life can be written as `A = R^TR`. How convenient. (If `A` can be written as `R^TR`, then it is positive definite.)</p>
          <p>The task now becomes finding a matrix `R` such that `A = R^TR.` (`A` has to be positive definite for this to work, so we'll assume `A` is positive definite.)</p>
          <p>Let `A = [[1, -2, -1],[-2, 8, 8],[-1, 8, 19]]`</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/find_cholesky_factor.gif">
          </div>
          <p>So `A = R^TR = [[1, -2, -1],[-2, 8, 8],[-1, 8, 19]] = [[1, 0, 0],[-2, 2, 0],[-1, 3, 3]] [[1, -2, -1],[0, 2, 3],[0, 0, 3]]`</p>
          <p class="box">To summarize: Finding `2` matrices `L` and `U` such that `A = LU` is generally hard. An easier approach is to find only `1` matrix `R` such that `A = R^TR.` By the nature of `R` (being upper-triangular) and `R^T` (being lower-triangular), we can still do forward and back substitution like we did for triangular systems. Doing this allows us to decompose `A` easily and <i>still</i> achieve the `n^2` time of doing forward and back substitution.</p>
          <p>Let's see long it takes to find `R`.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_1.gif">
          </div>
          <p>For the first column of `R`, we're doing operations on about `n^2/2` numbers in `R^T` (remember lower- and upper-triangular matrices have about `n^2/2` numbers).</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_2.gif">
          </div>
          <p>For the second column of `R`, we're also doing operations on about `n^2/2` numbers in `R^T`.</p>
          <p>This pattern repeats for all `n` columns of `R`. For each column, we're doing operations on <i>about</i> `n^2` numbers. Since there are `n` columns, we're doing operations on about `n*n^2 = n^3` numbers.</p>
          <p><b>So finding the Cholesky factor `R` requires `n^3` operations.</b> Wait. That sounds bad. Inverting a matrix took `n^3` operations and that was considered bad. So if finding `R` takes about as long as it does to find `A^-1`, why bother with Cholesky decomposition at all?</p>
          <p>Well, so far, all of this was done to solve `Ax = b`. Just <i>one</i> problem. What if we changed `b`? What if we were trying to solve `Ax = c?` Or `Ax = d?` (All using the same matrix `A`.) <b>If we had to solve multiple problems using the same matrix `A`, we would have to repeatedly do `n^3` operations to find the inverse* for each problem.</b> If we instead found the Cholesky decomposition first, we would have `R^T` and `R` stored in the computer ready to use for each problem and <i>only</i> have to do forward and back substitution for each problem. <b>This means for one problem, it will take `n^3` operations. But for future, subsequent problems, it will <i>only</i> take `n^2` operations</b>.</p>
          <p>*The actual method beginning linear algebra students would apply to solve `Ax = b` would probably be Gaussian Elimination, which still takes `n^3` operations. (When I say "beginning" I do not imply that I have more knowledge in linear algebra than they do lol.)</p>
          <div class="box">
            <p><b>The story thus far:</b> Finding `A^-1` takes too long `(n^3)`. Finding `L` and `U` such that `A = LU` is hard, but faster `(n^2)`. Finding `R` such that `A = R^TR` is easier, and potentially just as fast.</p>
            <p><b>Time complexity for Cholesky decomposition:</b> `n^3`</p>
          </div>
          <h2>Banded Matrices</h2>
          <p>Things get better if `A` has a bunch of `0`'s (if `A` has a lot of `0`'s, it is called "sparse".) Much like most matrices have a Cholesky factor, most matrices are also large and sparse. A specific type of sparse matrix - a banded matrix - actually allows us to perform Cholesky decomposition faster.</p>
          <p>(For those keeping track, all the assumptions we've made about `A` up to this point are that `A` is positive definite (can be split into `R^TR`) and banded (sparseness is implied). Again, `A` sounds super specific, but these type of matrices show up a lot apparently.)</p>
          <p>This is an example of a banded matrix: (notice the "band" of numbers across the diagonal and the `0`'s)</p>
          <div class="math">
            <p>`[[color(red)(2), color(red)(1), 0, 0, 0],[color(red)(1), color(red)(2), color(red)(1), 0, 0],[0, color(red)(1), color(red)(2), color(red)(1), 0],[0, 0, color(red)(1), color(red)(2), color(red)(1)],[0, 0, 0, color(red)(1), color(red)(2)]]`</p>
          </div>
          <p>It has bandwidth `3` (how wide the band is) and semiband `1` (half of the width of the band). In general, a banded matrix has semiband `s` and bandwidth `2s+1`.</p>
          <p>It turns out that there's an important result involving banded matrices: if `A` is positive definite and banded with semiband `s`, then its Cholesky factor `R` is also banded with semiband `s`. This is important, because if `A` had a bunch of `0`'s, then R will have a bunch of `0`'s too. These `0`'s mean less operations we have to do to find out what `R` is.</p>
          <p>Recall the process for finding `R` if `A` is not banded:</p>
          <div class="math">
            <!-- <p>`[[2, 1, 4, 9, 3],[1, 2, 1, 3, 2],[5, 1, 4, 1, 2],[3, 8, 1, 2, 1],[6, 8, 9, 1, 2]] = [[r_(1,1), 0, 0, 0, 0],[r_(1,2), r_(2,2), 0, 0, 0],[r_(1,3), r_(2,3), r_(3,3), 0, 0],[r_(1,4), r_(2,4), r_(3,4), r_(4,4), 0],[r_(1,5), r_(2,5), r_(3,5), r_(4,5), r_(5,5)]] [[r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)],[0, r_(2,2), r_(2,3), r_(2,4), r_(2,5)],[0, 0, r_(3,3), r_(3,4), r_(3,5)],[0, 0, 0, r_(4,4), r_(4,5)],[0, 0, 0, 0, r_(5,5)]]`</p> -->
            <img class="img-fluid" src="../pictures/lin_alg/find_r_nonbanded.gif">
          </div>
          <p>Notice how we have to go through all the rows of `A` to calculate `r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)`</p>
          <p>Now let's see what happens when `A` is banded:</p>
          <div class="math">
            <!-- <p>`[[2, 1, 0, 0, 0],[1, 2, 1, 0, 0],[0, 1, 2, 1, 0],[0, 0, 1, 2, 1],[0, 0, 0, 1, 2]] = [[r_(1,1), 0, 0, 0, 0],[r_(1,2), r_(2,2), 0, 0, 0],[r_(1,3), r_(2,3), r_(3,3), 0, 0],[r_(1,4), r_(2,4), r_(3,4), r_(4,4), 0],[r_(1,5), r_(2,5), r_(3,5), r_(4,5), r_(5,5)]] [[r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)],[0, r_(2,2), r_(2,3), r_(2,4), r_(2,5)],[0, 0, r_(3,3), r_(3,4), r_(3,5)],[0, 0, 0, r_(4,4), r_(4,5)],[0, 0, 0, 0, r_(5,5)]]`</p> -->
            <img class="img-fluid" src="../pictures/lin_alg/find_r_banded.gif">
          </div>
          <p>We only need to go through the first `2` rows of `A`. Why is that? Let's look at the `3^(rd)` row where we solve for the value of `r_(1,3)`. The equation is</p>
          <div class="math">
            <p>`r_(1,3)*r_(1,1) = 0`</p>
          </div>
          <p>Regardless of what `r_(1,1)` is*, `r_(1,3)` has to be `0`. The same holds true for the rest of the rows below. We know those values are going to be `0`, so why bother calculating them?</p>
          <p>*`r_(1,1)` will not be `0` because we assumed `A` was positive definite. This means the diagonal of `A` will be positive numbers. Solving for `r_(1,1)*r_(1,1) = #` cannot result in `r_(1,1) = 0`.</p>
          <p>Recognizing those `0`'s allows us to skip over numbers in each row. That means, in each row, we only have to perform operations on about `2s+1` (defined earlier as the width of the band) numbers. However, each number in that row requires about `s` numbers for the computation*. Therefore, for each row, we have a total of about `s^2` operations. With `n` rows, we have about `ns^2` total operations.</p>
          <div class="box">
            <p>*A visual for the above statement:</p>
            <div class="math">
              <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_additional_1.gif">
              <p>There are `n` rows.</p>
              <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_additional_2.gif">
              <p>There are `s` numbers in each row.</p>
              <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_additional_3.gif">
              <p>Each number requires `s` previous numbers for the calculation</p>
            </div>
          </div>
          <p>So it takes about `ns^2` operations to find `R`. Compared to non-banded matrices, it's faster than the `n^3` operations it took to find `R`. Even better is that because of all the `0`'s, performing forward and back substitution is also very fast.</p>
          <p>Recall the important fact mentioned earlier: if `A` is positive definite and banded with semiband `s`, then its Cholesky factor `R` is also banded with semiband `s`.</p>
          <p>So solving `Rx = y` (back substitution) could look something like this: (remember `R` is upper-triangular)</p>
          <div class="math">
            <p>`[[2, 1, 0, 0, 0],[0, 2, 1, 0, 0],[0, 0, 2, 1, 0],[0, 0, 0, 2, 1],[0, 0, 0, 0, 2]] [[x_1],[x_2],[x_3],[x_4],[x_5]] = [[4],[5],[6],[10],[4]]`</p>
          </div>
          <p>Each row only has about `s` numbers. With `n` rows, there are a total of `ns` numbers. <b>This means back substitution only takes `ns` operations. The same reasoning holds for forward substitution.</b></p>
          <p><b>All in all, it takes about `ns^2 + 2ns` (or simply, `ns^2`) operations to solve `Ax = b` if we know that `A` is positive definite and banded.</b> That is significantly faster than the `n^3` operations to find `R` and the `n^2` operations to do forward and back substitution.</p>
          <div class="box">
            <p><b>The story thus far:</b> Cholesky decomposition was a powerful method, but it still took `n^3` operations to find `R`. If we know `A` will be banded (and positive definite), then we can take advantage of its unique structure. The result is that it only takes `ns^2` operations to find `R` and another `ns` operations to do forward and back substitution to solve `Ax = b`.</p>
            <p><b>Time complexity of Cholesky decomposition with banded matrices:</b> `ns^2`</p>
          </div>
          <h2>Iterative Methods: Overview</h2>
          <p>Cholesky decomposition with banded matrices is nice. But it's only fast when the matrices are banded, which isn't always going to be the case. However, let's consider a different scenario. Let's suppose that we have such a giant matrix, that it takes a day or something to complete. If we stopped our program while it was in the middle of doing Cholesky, we would end up with nothing. It would be nice to be able to stop early and see if the answer we have so far is close to the real answer. That's what iterative methods allow us to do.</p>
          <p>Iterative methods start with an initial guess, `x^((0))`. Then they modify the guess to get a new guess, `x^((1)).` This repeats until the guess is (hopefully) close to the real answer.</p>
          <p>This probably seems like an unusual way of solving `Ax = b.` We're starting with some random guess and we end up with something that's not exactly the answer. It's also possible that we just keep guessing and never actually get close to the answer. But there are some advantages of using iterative methods. As mentioned before, we can stop early and get some sort of answer. <b>More importantly, iterative methods become fast when matrices are really large and sparse. And they don't need to be banded for this to work.</b></p>
          <div class="box">
            <p><b>The story thus far:</b> One way of approaching the `Ax = b` problem is to solve for the exact solution (e.g. by Cholesky decomposition). Another way of approaching the problem is to <i>approximate</i> the solution instead of getting the exact solution. This is sometimes faster and more applicable to general matrices.</p>
          </div>
          <h2>Iterative Methods: Jacobi</h2>
          <p>So we start with an initial guess - either a random guess or an educated one - and get a new guess. How do we get the new guess?</p>
          <p>Let's look at `Ax = b`.</p>
          <div class="math">
            <p>`[[a_(1,1), a_(1,2), a_(1,3)],[a_(2,1), a_(2,2), a_(2,3)],[a_(3,1), a_(3,2), a_(3,3)]] [[x_1],[x_2],[x_3]] = [[b_1],[b_2],[b_3]]`</p>
          </div>
          <p>If we wanted to get `x_1`, we would do:</p>
          <div class="math">
            <p>`a_(1,1)*x_1 + a_(1,2)*x_2 + a_(1,3)*x_3 = b_1`</p>
            <p>`a_(1,1)*x_1 = b_1 - a_(1,2)*x_2 - a_(1,3)*x_3`</p>
            <p>`x_1 = (b_1 - a_(1,2)*x_2 - a_(1,3)*x_3) / a_(1,1)`</p>
          </div>
          <p>By similar reasoning, `x_2` and `x_3` would be:</p>
          <div class="math">
            <p>`x_2 = (b_2 - a_(2,1)*x_1 - a_(2,3)*x_3) / a_(2,2)`</p>
            <p>`x_3 = (b_3 - a_(3,1)*x_1 - a_(3,2)*x_2) / a_(3,3)`</p>
          </div>
          <p>Generalizing this, we could say that to get `x_i`, we would do:</p>
          <div class="math">
            <p>`x_i = (b_i - sum_(j != i) a_(i,j)*x_j) / a_(i,i)`</p>
          </div>
          <p>This is the equation to get the exact answer. Since we're working with guesses and not exact answers, we need to modify this equation slightly:</p>
          <div class="math">
            <p>`x_i^((k+1)) = (b_i - sum_(j != i) a_(i,j)*x_j^((k))) / a_(i,i)`</p>
          </div>
          <p>On the `k^(th)` iteration, we have our `k^(th)` guess. We get our next guess (the `k+1^(st)` guess) by plugging in our `k^(th)` guess into that equation.</p>
          <p>This is Jacobi's method.</p>
          <p>How long does it take? Let's first look at how it performs on a full matrix.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/analyze_jacobi_full.gif">
          </div>
          <p>There are `n` rows. For each row, we have to multiply and add `n` numbers. So there are `n^2` operations in total.</p>
          <p>Doesn't seem to be too great. But what about if `A` was sparse? (with about `s` nonzero numbers per row)</p>
          <div class="math">
            <p>`[[0, a_(1,2), 0],[a_(2,1), 0, 0],[0, 0, a_(3,3)]] [[x_1],[x_2],[x_3]] = [[b_1],[b_2],[b_3]]`</p>
          </div>
          <p>Now, there are only `s` elements per row. So we only need to do `ns` operations in total. Recall that Cholesky decomposition on a banded matrix also took roughly `ns` operations (`ns^2` operations). <b>But it was only that fast when the matrix was banded. Here, with iterative methods, we achieved `ns` time without needing the matrix to be banded.</b></p>
          <h2>Iterative Methods: Gauss-Seidel</h2>
          <p>The Gauss-Seidel method is pretty similar to the Jacobi method, but it is slightly smarter. Recall the equation for Jacobi:</p>
          <div class="math">
            <p>`x_i^((k+1)) = (b_i - sum_(j != i) a_(i,j)*x_j^((k))) / a_(i,i)`</p>
          </div>
          <p>Another way to write this is:</p>
          <div class="math">
            <p>`x_i^((k+1)) = (b_i - sum_(j < i) a_(i,j)*x_j^((k)) - sum_(j > i) a_(i,j)*x_j^((k))) / a_(i,i)`</p>
          </div>
          <p>Notice the summation with `j < i`. That sum is going through all the values for which we have already made guesses. (That equation is giving us the new guess for the `i^(th)` number on the `k+1^(st)` iteration. So surely, we had to have made guesses for all the numbers before `i` on the `k+1^(st)` iteration right?) So instead of using an old guess, why not use the new guesses we just recently obtained?</p>
          <div class="math">
            <p>`x_i^((k+1)) = (b_i - sum_(j < i) a_(i,j)*x_j^((k+1)) - sum_(j > i) a_(i,j)*x_j^((k))) / a_(i,i)`</p>
          </div>
          <p>This is the Gauss-Seidel method.</p>
          <p>The Gauss-Seidel method should take about the same time as Jacobi's method since they are pretty much almost doing the same thing.</p>
          <div class="box">
            <p><b>The story thus far:</b> Two iterative methods that approximate the solution to `Ax = b` are Jacobi's method and the Gauss-Seidel method. Both of these start with an initial random guess and generate new guesses based on our old guesses.</p>
            <p><b>Time complexity for Jacobi/Gauss-Seidel:</b> `n^2` for full matrices, `n` for sparse matrices (`ns` if the sparseness depends on the size of the matrix)</p>
          </div>
        </div>
      </div>
    </div>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
