<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
    -->
    <title>Numerical Analysis: A Linear Algebra Story</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="lin_alg.css" rel="stylesheet">
  </head>
  <body>
    <a href="tutorials.html"><i id="back" class="fas fa-long-arrow-alt-left fa-2x"></i></a>
    <div class="container">
      <h1>Numerical Analysis: A Linear Algebra Story</h1>
      <div class="row">
        <div class="col-12">
          <p class="box">Note: This story assumes the reader has a basic understanding of matrices. (e.g. <a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html" target="_blank">matrix multiplication</a>, <a href="http://www.sosmath.com/matrix/matinv/matinv.html" target="_blank">invertibility</a>, <a href="http://www.mathwords.com/t/transpose_of_a_matrix.htm" target="_blank">transposes</a>). Familiarity with Big O notation helps, but isn't necessary to follow along.</p>
          <p>Linear algebra is all about solving `Ax = b,` where, usually, `A` is an `n xx n` matrix, `x` is an `n xx 1` matrix, and `b` is an `n xx 1` matrix. Specifically, given `A` and `b,` find `x`. In other words, if the problem is</p>
          <div class="math">
            <p>`[[a_(1,1), a_(1,2), ..., a_(1,n)],[a_(2,1), a_(2,2), ..., a_(2,n)],[..., ..., ..., ...],[a_(n,1), a_(n,2), ..., a_(n,n)]] [[x_1], [x_2], [...], [x_n]] = [[b_1], [b_2], [...], [b_n]]`</p>
          </div>
          <p>where we know all the `a` and `b` values, what values of `x` make that true?</p>
          <p>Concretely, what values of `x_1, x_2,` and `x_3` make that equation true?</p>
          <div class="math">
            <p>`[[1, 4, 1],[8, 2, 0],[5, 6, 2]][[x_1],[x_2],[x_3]] = [[24],[32],[49]]`</p>
          </div>
          <p>Of course, we could try plugging in random values for x and hope that they work. But that would take forever. Even if we did it with a computer, it would be impractical to do so all the time.</p>
          <h2>Finding `A^-1`</h2>
          <p>A <i>slightly</i> better approach would be to "isolate" the `x`. As from elementary algebra, we could solve for `x` in `5x = 10` by dividing both sides by `5`. So, for `Ax = b`, why not "divide" both sides by `A`? (More formally known as taking the inverse). There are some special cases in which it is impossible to take the inverse of `A`, so for simplicity, we avoid those cases and assume it is possible, i.e., `A` is "invertible".</p>
          <p>In theory, the answer really is that simple. Multiply both sides (on the left) by the inverse of `A` (denoted by `A^-1`) to get `x = A^-1b`. Done. In practice, though, this process is actually pretty complicated and time-consuming, <i>especially</i> when done by hand. In fact, doing it by hand is so nightmarish, let's assume we have a computer program that inverts and multiplies matrices for us. It turns out that even with a computer doing millions of operations in mere milliseconds, <b>inverting a matrix and then multiplying is relatively time-consuming</b>.</p>
          <p>It's natural to assume that if a program needs to do a lot of operations `(+,-,xx,//,sqrt())`, it will take longer than one that needs to do less operations. So <b>we'll consider how long inverting a matrix takes by looking at how many operations it takes.</b> The process is complicated (for me to know/explain), but it turns out that it generally takes `n^3` operations. This is quite a lot of operations. It also means that if you double the size of the matrix, you have to do `8` times more operations to find its inverse. (`2 xx 2` matrix: `8` operations, `4 xx 4` matrix: `64` operations) Most matrices in real life are really large. (`1,000,000 xx 1,000,000` is likely a conservative estimate. Imagine `1,000,000^3` operations.)</p>
          <div class="box">
            <p><b>The story thus far</b>: We're trying to solve `Ax = b`. (Given `A` and `b`, find `x`.) In theory, the simplest approach is to multiply both sides by `A^-1` to get `x = A^-1b`. In practice, this approach is not simple at all. Compared to other methods of solving this problem, finding `A^-1` is time-consuming. (And also space-inefficient, but I'll disregard it in this story.)</p>
            <p><b>Time complexity of finding `A^-1`</b>: `n^3`</p>
          </div>
          <h2>Triangular Systems</h2>
          <p>Let's assume `A` can be "split up" into two matrices: one lower-triangular matrix (denoted by `L`) and one upper-triangular matrix (denoted by `U`). So `A = LU:`</p>
          <div class="math">
            <p>`[[a_(1,1), a_(1,2), ..., a_(1,n)],[a_(2,1), a_(2,2), ..., a_(2,n)],[..., ..., ..., ...],[a_(n,1), a_(n,2), ..., a_(n,n)]] = [[l_(1,1), 0, ..., 0],[l_(2,1), l_(2,2), 0, ...],[..., ..., ..., 0],[l_(n,1), l_(n,2), ..., l_(n,n)]] [[u_(1,1), u_(1,2), ..., u_(1,n)],[0, u_(2,2), ..., u_(2,n)],[..., 0, ..., ...],[0, ..., 0, u_(n,n)]]`.</p>
          </div>
          <p class="box">Note: Lower-triangular means `0`'s on top right and numbers on bottom left. Upper-triangular means numbers on top right and `0`'s on bottom left. See example below.</p>
          <p>For example,</p>
          <div class="math">
            <p>`[[6, 4, 2, 2],[-3, 0, 3, 5],[9, 7, 7, 5],[12, 9, 12, 16]] = [[2, 0, 0, 0],[-1, 2, 0, 0],[3, 1, -1, 0],[4, 1, -3, 3]] [[3, 2, 1, 1],[0, 1, 2, 3],[0, 0, -2, 1],[0, 0, 0, 4]]`</p>
          </div>
          <p>The problem now becomes `(A)x = b => (LU)x = b.` Where do go from here? The answer lies in the special property of lower- and upper-triangular matrices. (Namely, the zeros.)</p>
          <p>First of all, it is quite easy to solve something like `Lx = b`. For example,</p>
          <div class="math">
            <p>`[[color(red)(5), color(red)(0), color(red)(0)],[color(blue)(2), color(blue)(-4), color(blue)(0)],[color(green)(1), color(green)(2), color(green)(3)]] [[x_1],[x_2],[x_3]] = [[color(red)(15)],[color(blue)(-2)],[color(green)(10)]]`</p>
            <p>`color(red)(5)x_1 + color(red)(0)x_2 + color(red)(0)x_3 = color(red)(15) => x_1 = 3`</p>
            <p>`color(blue)(2)x_1 + color(blue)(-4)x_2 + color(blue)(0)x_3 = color(blue)(-2) => x_2 = 2` (plugging in `3` for `x_1`)</p>
            <p>`color(green)(1)x_1 + color(green)(2)x_2 + color(green)(3)x_3 = color(green)(10) => x_3 = 1` (plugging in `3` for `x_1` and `2` for `x_2`)</p>
            <p>(<b>This process is known as forward substitution.</b>)</p>
          </div>
          <p>We have `LUx = b`. To take advantage of the easiness of forward substitution, let's view the problem as: `L(y) = b`, where `y = Ux`. Solving `Ly = b` like in the previous example, we easily obtain values for `y`. But how do we get `x`?</p>
          <p>It's also just as easy to solve something like `Ux = b`. For example,</p>
          <div class="math">
            <p>`[[color(green)(3), color(green)(2), color(green)(4)],[color(blue)(0), color(blue)(1), color(blue)(2)],[color(red)(0), color(red)(0), color(red)(3)]] [[x_1],[x_2],[x_3]] = [[color(green)(19)],[color(blue)(8)],[color(red)(9)]]`</p>
            <p>`color(red)(0)x_1 + color(red)(0)x_2 + color(red)(3)x_3 = color(red)(9) => x_3 = 3`</p>
            <p>`color(blue)(0)x_1 + color(blue)(1)x_2 + color(blue)(2)x_3 = color(blue)(8) => x_2 = 2` (plugging in `3` for `x_3`)</p>
            <p>`color(green)(3)x_1 + color(green)(2)_x2 + color(green)(4)x_3 = color(green)(19) => x_1 = 1` (plugging in `3` for `x_3` and `2` for `x_2`)</p>
            <p>(<b>This process is known as back substitution.</b>)</p>
          </div>
          <p>Notice before, we defined `y = Ux`. We have `y` (from solving `Ly = b`), and we have `U` (from assuming it was possible to split `A = LU`). So like in the previous example, it is possible to solve for `x`. Let's see how this plays out with real numbers.</p>
          <p>Let `A = [[2, 1],[6, 5]]`, `b = [[4],[16]]`. Our goal is to find `x_1, x_2` such that `[[2, 1],[6, 5]] [[x_1],[x_2]] = [[4],[16]]`</p>
          <p>Let's also say we knew exactly how to split up `A`:</p>
          <div class="math">
            <p>`A = LU = [[1, 0],[3, 2]] [[2, 1],[0, 1]]`</p>
          </div>
          <p>The new problem becomes:</p>
          <div class="math">
            <p>`[[1, 0],[3, 2]] [[2, 1],[0, 1]] [[x_1],[x_2]] = [[4],[16]]`</p>
            <p>(1)</p>
          </div>
          <p>Let's let</p>
          <div class="math">
            <p>`[[y_1],[y_2]] = [[2, 1],[0, 1]] [[x_1],[x_2]]`</p>
            <p>(2)</p>
          </div>
          <p>so that</p>
          <div class="math">
            <p>`[[1, 0],[3, 2]] [[y_1],[y_2]] = [[4],[16]]`</p>
            <p>`1y_1 + 0y_2 = 4 => y_1 = 4`</p>
            <p>`3y_1 + 2y_2 = 16 => y_2 = 2`</p>
            <p>(3)</p>
          </div>
          <p>Now that we have our values for `y`, let's go back to (2):</p>
          <div class="math">
            <p>`[[4],[2]] = [[2, 1],[0, 1]] [[x_1],[x_2]]`</p>
            <p>`0x_1 + 1x_2 = 2 => x_2 = 2`</p>
            <p>`2x_1 + 1x_2 = 4 => x_1 = 1`</p>
            <p>(4)</p>
          </div>
          <p>And we have our values for x, as desired.</p>
          <p class="box">To summarize: We assumed `A` could be "split up" into two parts `L` and `U` (i.e., `A = LU`). With that, the problem became `Ax = b => LUx = b`. We took advantage of the fact that solving something like `Lx = b` was easy. So by letting `y = Ux`, we could solve `Ly = b`. Doing that gave us `y`, so we could get `x` by solving `Ux = y`.</p>
          <p>This process may look like it requires a lot of steps. <b>But it actually takes less operations than inverting a matrix.</b> Let's look at how many operations it takes to solve `Ly = b`. <b>We'll count operations by counting how many numbers we have to perform operations on.</b></p>
          <p>For the first row, there is `1` number. (By nature of matrix multiplication, the `0`'s don't count as numbers.)</p>
          <div class="math">
            <p>`[[color(red)(5), 0, 0],[2, -4, 0],[1, 2, 3]] [[color(red)(x_1)],[x_2],[x_3]] = [[15],[-2],[10]]`</p>
          </div>
          <p>For the second row, there are `2` numbers.</p>
          <div class="math">
            <p>`[[5, 0, 0],[color(red)(2), color(red)(-4), 0],[1, 2, 3]] [[color(red)(x_1)],[color(red)(x_2)],[x_3]] = [[15],[-2],[10]]`</p>
          </div>
          <p>This continues until the `n^(th)` row where there are `n` numbers.</p>
          <div class="math">
            <p>`[[5, 0, 0],[2, -4, 0],[color(red)(1), color(red)(2), color(red)(3)]] [[color(red)(x_1)],[color(red)(x_2)],[color(red)(x_3)]] = [[15],[-2],[10]]`</p>
          </div>
          <p>So there are `1 + 2 + ... + n ~~ n^2` total numbers* we have to perform operations on.</p>
          <p>*Per Big O notation, constants and lower power terms are dropped. It's actually `(n(n+1))/2 = (n^2 + n)/2` numbers.</p>
          <div class="box">
            <p>A picture for the above statement:</p>
            <div class="math">
              <p>`[[***,*,*,*,*],[***,***,*,*,*],[***,***,***,*,*],[***,***,***,***,*],[***,***,***,***,***]]`</p>
            </div>
            <p>Counting the number of stars, notice that it takes up about <i>half</i> of the area of the square. Hence, about `n^2/2` numbers.</p>
          </div>
          <p><b>This means we have to do <i>about</i> `n^2` operations to solve `Ly = b` (forward substitution).</b> Now we have to solve `Ux = y` (back substitution). By similar reasoning, `U` also has `n^2/2` numbers, so it also takes <i>about</i> `n^2` operations. <b>Altogether, the total number of operations to solve `LUx = b` is `n^2 + n^2 = 2n^2.`</b> For simplicity, we can consider it as `n^2` operations.</p>
          <p>This is <i>definitely</i> less than the `n^3` operations it took to find `A^-1.`</p>
          <div class="box">
            <p><b>The story thus far</b>: Finding `A^-1` simply takes too long `(n^3).` We found out that if we could split `A` into two parts, `L` and `U`, it would only take about `n^2` operations to solve for `x`.
            <p><b>Time complexity for forward and back substitution</b>: `n^2`</p>
          </div>
          <h2>Cholesky Decomposition</h2>
          <p>It turns out that it generally isn't easy finding matrices `L` and `U` such that `A = LU.` But splitting `A` into triangular matrices made solving `Ax = b` so fast though. Relatively fast anyway. Is it possible to still use this idea of splitting matrices into `2` parts, but with the parts (relatively) easy to find?</p>
          <p>It is, but we're gonna have to assume more things about the matrix `A`. Let's assume that there exists an upper-triangular matrix `R` such that `A = R^TR.` <b>So now, instead of finding two matrices `L` and `U` such that `A = LU,` we only need to find `1` matrix `R` such that `A = R^TR.` (`R` is called the "Cholesky factor". This process is called "Cholesky decomposition".)</b> Since `R` is upper-triangular, `R^T` will be lower-triangular, so we can still apply the forward and back substitution methods as before.</p>
          <p class="box">Note: `T` means matrix transpose.</p>
          <p>For example, `A = [[1, 1, 1],[1, 2, 2],[1, 2, 3]]` can be split up into:</p>
          <div class="math">
            <p>`[[1, 1, 1],[1, 2, 2],[1, 2, 3]] = [[1, 0, 0],[1, 1, 0],[1, 1, 1]] [[1, 1, 1],[0, 1, 1],[0, 0, 1]]`</p>
          </div>
          <p>where `R = [[1, 1, 1],[0, 1, 1],[0, 0, 1]]`, and `R^T = [[1, 0, 0],[1, 1, 0],[1, 1, 1]]`</p>
          <p>I know we made a lot of assumptions about `A`, so all of this looks like it'll only work for super-specific cases. However, it turns out that most of the useful matrices used in real life can be written as `A = R^TR`. How convenient. (If `A` can be written as `R^TR`, then it is symmetric, positive definite.)</p>
          <p>The task now becomes finding a matrix `R` such that `A = R^TR.` (`A` has to be symmetric, positive definite for this to work, so we'll assume `A` is symmetric, positive definite.)</p>
          <p>Let `A = [[1, -2, -1],[-2, 8, 8],[-1, 8, 19]]`</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/find_cholesky_factor.gif">
          </div>
          <p>So `A = R^TR = [[1, -2, -1],[-2, 8, 8],[-1, 8, 19]] = [[1, 0, 0],[-2, 2, 0],[-1, 3, 3]] [[1, -2, -1],[0, 2, 3],[0, 0, 3]]`</p>
          <p class="box">To summarize: Finding `2` matrices `L` and `U` such that `A = LU` is generally hard. An easier approach is to find only `1` matrix `R` such that `A = R^TR.` By the nature of `R` (being upper-triangular) and `R^T` (being lower-triangular), we can still do forward and back substitution like we did for triangular systems. Doing this allows us to decompose `A` easily and <i>still</i> achieve the `n^2` time of doing forward and back substitution.</p>
          <p>Let's see long it takes to find `R`.</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_1.gif">
          </div>
          <p>For the first column of `R`, we're doing operations on about `n^2/2` numbers in `R^T` (remember lower- and upper-triangular matrices have about `n^2/2` numbers).</p>
          <div class="math">
            <img class="img-fluid" src="../pictures/lin_alg/analyze_cholesky_2.gif">
          </div>
          <p>For the second column of `R`, we're also doing operations on about `n^2/2` numbers in `R^T`.</p>
          <p>This pattern repeats for all `n` columns of `R`. For each column, we're doing operations on <i>about</i> `n^2` numbers. Since there are `n` columns, we're doing operations on about `n*n^2 = n^3` numbers.</p>
          <p><b>So finding the Cholesky factor `R` requires `n^3` operations.</b> Wait. That sounds bad. Inverting a matrix took `n^3` operations and that was considered bad. So if finding `R` takes about as long as it does to find `A^-1`, why bother with Cholesky decomposition at all?</p>
          <p>Well, so far, all of this was done to solve `Ax = b`. Just <i>one</i> problem. What if we changed `b`? What if we were trying to solve `Ax = c?` Or `Ax = d?` (All using the same matrix `A`.) <b>If we had to solve multiple problems using the same matrix `A`, we would have to repeatedly do `n^3` operations to find the inverse* for each problem.</b> If we instead found the Cholesky decomposition first, we would have `R^T` and `R` stored in the computer ready to use for each problem and <i>only</i> have to do forward and back substitution for each problem. <b>This means for one problem, it will take `n^3` operations. But for future, subsequent problems, it will <i>only</i> take `n^2` operations</b>.</p>
          <p>*The actual method beginning linear algebra students would apply to solve `Ax = b` would probably be Gaussian Elimination, which still takes `n^3` operations. (When I say "beginning" I do not imply that I have more knowledge in linear algebra than they do lol.)</p>
          <div class="box">
            <p><b>The story thus far:</b> Finding `A^-1` takes too long `(n^3)`. Finding `L` and `U` such that `A = LU` is hard, but faster `(n^2)`. Finding `R` such that `A = R^TR` is easier, and potentially just as fast.</p>
            <p><b>Time complexity for Cholesky decomposition:</b> `n^3`</p>
          </div>
          <h2>Banded Matrices</h2>
          <p>Things get better if `A` has a bunch of `0`'s (if `A` has a lot of `0`'s, it is called "sparse".) Much like most matrices have a Cholesky factor, most matrices are also large and sparse. A specific type of sparse matrix - a banded matrix - actually allows us to perform Cholesky decomposition faster.</p>
          <p>(For those keeping track, all the assumptions we've made about `A` up to this point are that `A` is symmetric, positive definite, and banded (sparseness is implied). Again, `A` sounds super specific, but these type of matrices show up a lot apparently.)</p>
          <p>This is an example of a banded matrix: (notice the "band" of numbers across the diagonal and the `0`'s)</p>
          <div class="math">
            <p>`[[color(red)(2), color(red)(1), 0, 0, 0],[color(red)(1), color(red)(2), color(red)(1), 0, 0],[0, color(red)(1), color(red)(2), color(red)(1), 0],[0, 0, color(red)(1), color(red)(2), color(red)(1)],[0, 0, 0, color(red)(1), color(red)(2)]]`</p>
          </div>
          <p>It has bandwidth `3` (how wide the band is) and semiband `1` (half of the width of the band). In general, a banded matrix has semiband `s` and bandwidth `2s+1`.</p>
          <p>It turns out that there's an important result involving banded matrices: if `A` is symmetric, positive definite, and banded with semiband `s`, then its Cholesky factor `R` is also banded with semiband `s`. This is important, because if `A` had a bunch of `0`'s, then R will have a bunch of `0`'s too. These `0`'s mean less operations we have to do to find out what `R` is.</p>
          <p>Recall the process for finding `R` if `A` is not banded:</p>
          <div class="math">
            <p>`[[2, 1, 4, 9, 3],[1, 2, 1, 3, 2],[5, 1, 4, 1, 2],[3, 8, 1, 2, 1],[6, 8, 9, 1, 2]] = [[r_(1,1), 0, 0, 0, 0],[r_(1,2), r_(2,2), 0, 0, 0],[r_(1,3), r_(2,3), r_(3,3), 0, 0],[r_(1,4), r_(2,4), r_(3,4), r_(4,4), 0],[r_(1,5), r_(2,5), r_(3,5), r_(4,5), r_(5,5)]] [[r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)],[0, r_(2,2), r_(2,3), r_(2,4), r_(2,5)],[0, 0, r_(3,3), r_(3,4), r_(3,5)],[0, 0, 0, r_(4,4), r_(4,5)],[0, 0, 0, 0, r_(5,5)]]`</p>
          </div>
          <p>Notice how we have to go through all the rows of `A` to calculate `r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)`</p>
          <p>Now let's see what happens when `A` is banded:</p>
          <div class="math">
            <p>`[[2, 1, 0, 0, 0],[1, 2, 1, 0, 0],[0, 1, 2, 1, 0],[0, 0, 1, 2, 1],[0, 0, 0, 1, 2]] = [[r_(1,1), 0, 0, 0, 0],[r_(1,2), r_(2,2), 0, 0, 0],[r_(1,3), r_(2,3), r_(3,3), 0, 0],[r_(1,4), r_(2,4), r_(3,4), r_(4,4), 0],[r_(1,5), r_(2,5), r_(3,5), r_(4,5), r_(5,5)]] [[r_(1,1), r_(1,2), r_(1,3), r_(1,4), r_(1,5)],[0, r_(2,2), r_(2,3), r_(2,4), r_(2,5)],[0, 0, r_(3,3), r_(3,4), r_(3,5)],[0, 0, 0, r_(4,4), r_(4,5)],[0, 0, 0, 0, r_(5,5)]]`</p>
          </div>
          <p>We only need to go through the first `2` rows of `A`. Why is that? Let's look at the `3^(rd)` row where we solve for the value of `r_(1,3)`. The equation is</p>
          <div class="math">
            <p>`r_(1,3)*r_(1,1) = 0`</p>
          </div>
          <p>Regardless of what `r_(1,1)` is*, `r_(1,3)` has to be `0`. The same holds true for the rest of the rows below. We know those values are going to be `0`, so why bother calculating them?</p>
          <p>*`r_(1,1)` will not be `0` because we assumed `A` was positive definite. This means the diagonal of `A` will be positive numbers. Solving for `r_(1,1)*r_(1,1) = #` cannot result in `r_(1,1) = 0`.</p>
          <p>Recognizing those `0`'s allows us to skip over numbers in each row. That means, in each row, we only have to perform operations on about `2s+1` (defined earlier as the width of the band) numbers. However, each number in that row requires about `s` numbers for the computation*. Therefore, for each row, we have a total of about `s^2` operations. With `n` rows, we have about `ns^2` total operations.</p>
          <div class="box">
            <p>*Looking at the example matrix above, calculating the value of `r_(3,3)` requires knowing the value of `r_(2,3)` `(r_(2,3)*r_(2,3) + r_(3,3)*r_(3,3) = 2)`. If `s` was greater than `1`, we would have needed the value of `r_(1,3)` as well `(r_(1,3)*r_(1,3) + r_(2,3)*r_(2,3) + r_(3,3)*r_(3,3)).` This is because if the semiband was longer, `a_(3,1)` (`3^(rd)` row, `1^(st)` column of `A`) would not have been `0`, so `r_(1,3)` would not have been `0` (and therefore, would be necessary for the computation). Calculating the value of `r_(4,4)` requires `r_(3,4), r_(2,4), ...` Simply put, in order to calculate any `r` value, we need to multiply and add their previous `r` values (there are about `s` previous values). This is why each of the `s` numbers in each row requires `s` operations to compute.</p>
          </div>
          <p>So it takes about `ns^2` operations to find `R`. Compared to non-banded matrices, it's faster than the `n^3` operations it took to find `R`. Even better is that because of all the `0`'s, performing forward and back substitution is also very fast.</p>
          <p>Recall the important fact mentioned earlier: if `A` is symmetric, positive definite, and banded with semiband `s`, then its Cholesky factor `R` is also banded with semiband `s`.</p>
          <p>So solving `Rx = y` (back substitution) could look something like this: (remember `R` is upper-triangular)</p>
          <div class="math">
            <p>`[[2, 1, 0, 0, 0],[0, 2, 1, 0, 0],[0, 0, 2, 1, 0],[0, 0, 0, 2, 1],[0, 0, 0, 0, 2]] [[x_1],[x_2],[x_3],[x_4],[x_5]] = [[4],[5],[6],[10],[4]]`</p>
          </div>
          <p>Like before, each row only has about `s` numbers, each requiring `s` previous values for the computation. With `n` rows, there are a total of `ns^2` operations. <b>This means back substitution only takes `ns^2` operations. The same reasoning holds for forward substitution.</b></p>
          <p>All in all, it takes about `3ns^2` (or simply, `n`) operations to solve `Ax = b` if we know that `A` is symmetric, positive definite, and banded. That is significantly faster than the `n^3` operations to find `R` and the `n^2` operations to do forward and back substitution.</p>
          <p>As a disclaimer, I could be wrong about the specific details of the time complexity. However, the end result is the same: `n` operations.</p>
          <div class="box">
            <p><b>The story thus far:</b> Cholesky decomposition was a powerful method, but it still took `n^3` operations to find `R`. If we know `A` will be banded, then we can take advantage of its unique structure. The result is that it only takes `ns^2` operations to find `R` and another `ns` operations to do forward and back substitution to solve `Ax = b`. `s` is usually relatively small compared to `n` so it's safe to say that the whole process takes `n` operations.</p>
            <p><b>Time complexity of Cholesky decomposition with banded matrices:</b> `n`</p>
          </div>
        </div>
      </div>
    </div>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
